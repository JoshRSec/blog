[{"content":"Here you can find all my previous posts.\n","date":"31 January 2026","externalUrl":null,"permalink":"/posts/","section":"Blog Posts","summary":"","title":"Blog Posts","type":"posts"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"I write about the things I’m building, learning, breaking and fixing across Computer Science and Cyber Security.\n","date":"31 January 2026","externalUrl":null,"permalink":"/","section":"Home","summary":"","title":"Home","type":"page"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":" From WordPress to Hugo: Rebuilding My Blog for Speed, Security, and Maintainability # For years, my blog lived on WordPress. It worked, but it never felt like something I fully controlled. Between PHP vulnerabilities, plugin bloat, performance issues, and the constant need for updates, the platform carried more risk and overhead than I wanted for a personal site.\nEventually, I decided it was time for a change - not just a redesign, but a complete rebuild on a platform that aligned with how I like to work: fast, secure, version‑controlled, and future‑proof. That’s what led me to Hugo.\nThis article walks through why I migrated, how I built a fully automated conversion pipeline, and what I learned along the way.\nWhy I Moved Away from WordPress # 1. Security Concerns # WordPress is built on PHP, and while the core team does a solid job, the ecosystem introduces unavoidable risks:\nPHP vulnerabilities can expose entire sites. Plugins - even reputable ones - frequently become attack vectors. A single outdated dependency can compromise the whole installation. Shared hosting environments amplify risk. For a personal blog, that level of exposure felt unnecessary. I wanted something static, immutable, and safe by design.\n2. Plugin Bloat and Fragility # WordPress sites tend to accumulate plugins over time:\nSEO plugins Image optimizers Syntax highlighters Backup tools Caching layers Theme‑specific helpers Each plugin adds code, complexity, and potential breakage. Updates sometimes fix issues, sometimes introduce new ones, and occasionally take down the entire site.\nI wanted a system where nothing ran on the server. No plugins. No PHP. No database. Low attack surface.\n3. Performance # Even with caching, WordPress can feel sluggish. Hugo, on the other hand, generates the entire site in milliseconds. The result is:\nInstant page loads Zero server processing No database queries Perfect Lighthouse scores Static HTML is hard to beat.\n4. Maintainability and Ownership # With WordPress, content lives in a database. With Hugo, every post becomes:\nEverything is:\nVersion‑controlled Portable Editable in any text editor Easy to back up Easy to automate It’s the difference between renting and owning.\nThe Challenge: WordPress Exports Are Messy # WordPress exports posts as XML, and inside that XML is a mixture of:\nHTML inside CDATA blocks WordPress shortcodes Embedded scripts Caption wrappers Escaped entities Inconsistent formatting Images wrapped in \u0026lt;a\u0026gt; tags Gists embedded via \u0026lt;script\u0026gt; tags A simple HTML‑to‑Markdown converter wasn’t going to cut it. I needed a custom migration pipeline.\nBuilding the Migration Pipeline # I wrote a PowerShell script that processes each post and outputs a clean Hugo content bundle. The pipeline handles:\n1. Cleaning HTML Entities # WordPress escapes everything. I unescaped:\n\u0026amp;lt; → \u0026lt; \u0026amp;gt; → \u0026gt; \u0026amp;amp; → \u0026amp; \u0026amp;quot; → \u0026quot; \u0026amp;#39; → ' This made the HTML parseable.\n2. Removing WordPress Caption Shortcodes # WordPress stores captions like this:\n[caption id=\u0026quot;...\u0026quot; align=\u0026quot;...\u0026quot;] \u0026lt;img ... /\u0026gt; Caption text here [/caption]\nIf you remove the shortcode wrapper incorrectly, you either:\nLose the image Keep the caption text Break the HTML The final solution extracts only the \u0026lt;img\u0026gt; tag, discarding the caption text entirely. This preserves the image while avoiding duplicated captions in Markdown.\n3. Converting Links # I rewrote:\n\u0026lt;a href=\u0026quot;URL\u0026quot;\u0026gt;text\u0026lt;/a\u0026gt;\ninto:\n[text](URL)\nBut with one critical rule:\nIf the link wraps an image, leave it alone.\nOtherwise, the image disappears before extraction.\n4. Extracting and Downloading Images # Every \u0026lt;img\u0026gt; tag is:\nParsed Downloaded Saved into the post’s img/ folder Replaced with a Markdown image reference This ensures the site is fully self‑contained and not dependent on external WordPress media URLs.\n5. Converting GitHub Gists # WordPress embeds Gists using \u0026lt;script\u0026gt; tags. Hugo uses:\n{{ \u0026lt; gist user id \u0026gt; }}\nThe script automatically converts these.\n6. Stripping Remaining HTML # Once images, links, and shortcodes are handled, the remaining HTML is safe to strip or convert.\n7. Generating Hugo Front Matter # Each post gets:\ntitle date slug categories tags aliases (for redirects) featured image Everything is clean, consistent, and ready for Hugo.\nThe Result: A Clean, Secure, Future‑Proof Blog # After running the script, every WordPress post became a tidy Hugo content bundle. Images were local, links were Markdown, captions were fixed, and the entire site built instantly.\nDeploying to GitHub Pages completed the transformation. Now my blog is:\nStatic Secure Fast Version‑controlled Easy to maintain No PHP. No plugins. Low attack surface.\nLessons Learned # 1. WordPress hides a lot of complexity # You don’t notice it until you try to leave.\n2. HTML is messy # Even CMS‑generated HTML is full of edge cases.\n3. Automation is worth the investment # Once the script was right, I could re‑run the entire migration in seconds.\n4. Hugo rewards structure # Content bundles, shortcodes, and Markdown make everything predictable.\nWould I Do It Again? Absolutely. # Moving from WordPress to Hugo wasn’t a one‑click migration - it was a project. But it gave me:\na faster site a more secure site a cleaner workflow full ownership of my content a setup I can maintain for years If you’re considering the same move, it’s absolutely worth it.\n","date":"31 January 2026","externalUrl":null,"permalink":"/posts/migrating-wordpress-to-hugo/","section":"Blog Posts","summary":"A clean, static rebuild of my blog using Hugo replaced the security risks, plugin bloat, and performance issues of WordPress with a fast, version‑controlled, fully automated setup; by building a custom migration pipeline to convert messy WordPress XML into structured Hugo content bundles—with local images, cleaned HTML, converted links, and consistent front matter—I ended up with a secure, future‑proof site that’s easier to maintain and far faster than anything I could achieve on WordPress.","title":"Migrating from Wordpress to Hugo","type":"posts"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/powershell/","section":"Tags","summary":"","title":"Powershell","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/categories/web-design/","section":"Categories","summary":"","title":"Web Design","type":"categories"},{"content":"","date":"31 January 2026","externalUrl":null,"permalink":"/tags/wordpress/","section":"Tags","summary":"","title":"Wordpress","type":"tags"},{"content":"","date":"29 January 2020","externalUrl":null,"permalink":"/tags/docx/","section":"Tags","summary":"","title":"Docx","type":"tags"},{"content":"","date":"29 January 2020","externalUrl":null,"permalink":"/tags/microsoft-word/","section":"Tags","summary":"","title":"Microsoft Word","type":"tags"},{"content":"","date":"29 January 2020","externalUrl":null,"permalink":"/categories/powershell/","section":"Categories","summary":"","title":"Powershell","type":"categories"},{"content":"Found myself in a situation I needed to search 20+ documents for a keyword and unfortunately Word cannot search files within a directory for keywords, only the current document.\nThankfully we can automate this with PowerShell.\nThe script below will:\nAccept a directory path and keyword Open each file found and search for the keyword If the file contains the keyword it will add to an array of files containing the keyword Output results A caveat with this approach is Word needs to be installed on the machine in order to open an instance and then search, but I would imagine if the machine is storing Word documents then it likely has Word installed!\n","date":"29 January 2020","externalUrl":null,"permalink":"/posts/searching-multiple-word-documents-for-keyword/","section":"Blog Posts","summary":"Found myself in a situation I needed to search 20+ documents for a keyword and unfortunately Word cannot search files within a directory for keywords, only the current document","title":"Searching multiple Word documents for keyword","type":"posts"},{"content":"","date":"29 January 2020","externalUrl":null,"permalink":"/categories/windows/","section":"Categories","summary":"","title":"Windows","type":"categories"},{"content":"","date":"29 January 2020","externalUrl":null,"permalink":"/tags/word/","section":"Tags","summary":"","title":"Word","type":"tags"},{"content":"","date":"19 December 2019","externalUrl":null,"permalink":"/tags/active-directory/","section":"Tags","summary":"","title":"Active Directory","type":"tags"},{"content":"This method may benefit from some social engineering but will require local Administrator on an machine within the network. Social engineering can be used to speedup the process of enticing a Domain Admin (DA) to login to a system and in-return provide us DA rights.\nShould you need to obtain local Administrator privileges and you have access to a machine which isn\u0026rsquo;t using Bit-locker, here is a guide on doing so (It wont take long).\nCreate a new scheduled task Ensure the \u0026ldquo;Run with highest privileges\u0026rdquo; checkbox is checked Change the user in which the task will run as to that of the target DA account\nSet the trigger to be when the target DA logs onto the machine\nSet the action to run \u0026rsquo;net.exe\u0026rsquo; -\u0026gt; add the parameter \u0026lsquo;group \u0026ldquo;Domain Admins\u0026rdquo; user /add /domain\u0026rsquo;\nNow we lay in wait. Perhaps you have Admins which use their DA accounts for remote support, you could always raise a ticket and get them to remote onto the machine. You could also set this trap on a machine which you know the DAs will login to (Provided you have local Admin).\nThis method can be mitigated through blocking Domain Admins from logging into workstations by group policy and following the principle of least privilege.\n","date":"19 December 2019","externalUrl":null,"permalink":"/posts/escalating-privileges-to-domain-admin/","section":"Blog Posts","summary":"A quick overview of how local admin rights on a workstation can be misused to escalate privileges when a Domain Admin logs in, along with the reminder that proper least‑privilege policies prevent this risk.","title":"Escalating Privileges to Domain Admin","type":"posts"},{"content":"","date":"19 December 2019","externalUrl":null,"permalink":"/categories/security/","section":"Categories","summary":"","title":"Security","type":"categories"},{"content":"","date":"19 December 2019","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"19 December 2019","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":"","date":"18 December 2019","externalUrl":null,"permalink":"/tags/bitlocker/","section":"Tags","summary":"","title":"Bitlocker","type":"tags"},{"content":"This is an old but still relevant method of gaining access to an local Administrator account on any windows machine running Vista on-wards.\nNote: This method will only work when Bitlocker is not enabled on the machine.\nYou can either use a live CD or Windows PE (From Windows installation media), this guide will use Windows PE.\nDownload Windows Media Creation Tool and make a USB installer by following the instructions when launching the .exe (Any version is fine as we are not installing Windows). Change the boot priority of the device to use the USB before the OS Disk Launch the Windows Installation Media and press SHIFT+F10 to launch a CMD prompt Find the Windows OS volume by looking through the different drives, e.g.\nType \u0026ldquo;C:\u0026rdquo; -\u0026gt; \u0026ldquo;Dir\u0026rdquo; -\u0026gt; Replacing C: With D: etc\u0026hellip;\nOnce you have found the OS drive, Enter \u0026ldquo;cd X:/Windows/System32\u0026rdquo; (X: being the OS drive identified earlier)\nWe are going to replace the sticky keys executable with CMD, so when sticky keys is launched we open CMD instead\nMake a backup of sticky keys with: copy sethc.exe ..\nOverwrite sticky keys with CMD by entering: copy cmd.exe sethc.exe\nReboot the machine without launching the Installation Media\nOnce windows is loaded up, press SHIFT 5 times to launch a CMD\nBy entering \u0026ldquo;whoami\u0026rdquo; you can see we are running as system\nList the local users by typing \u0026ldquo;net user\u0026rdquo;\nYou can also run computer management if you prefer a GUI by entering: compmgmt.msc\nDepending on the target machines configuration you may see the \u0026ldquo;Administrator\u0026rdquo; account, however, if best practises are followed this may have been renamed.\nYou can check which groups an account is apart of by typing:\nnet user USERACCOUNT\nAssuming the Administator account hasn\u0026rsquo;t been renamed, you can activate it with:\nnet user Administrator /ACTIVE:Yes\nChange the password of the Administrator account with:\nnet user Administrator Password123\nYou can now login to the device with the password in which you defined above\nYou can revert the changed sethc.exe while logged in by opening a CMD as admin and entering:\nrobocopy c:\\windows c:\\windows\\system32 sethc.exe /B\n/B instructs robocopy to use the Backup API, otherwise the operation will fail due to insufficient privileges\nTo mitigate the risk the risk of this happening to a machine in your environment you should use BitLocker to prevent access to the OS drive being the machine has booted up.\n","date":"18 December 2019","externalUrl":null,"permalink":"/posts/gaining-entry-into-windows-with-administrative-permissions/","section":"Blog Posts","summary":"A high‑level look at how physical access to a non‑BitLocker‑protected Windows machine can allow someone to trigger a system‑level command prompt at the login screen and reset the local Administrator account, highlighting why full‑disk encryption is essential for preventing this risk.","title":"Gaining entry into Windows as Administrator","type":"posts"},{"content":"Whilst Sysinternals can be downloaded and ran from the location to which they are extracted to, there is more preferable way to install on a system in which they will be used more frequently.\nDownload Sysinternals from: https://docs.microsoft.com/en-us/sysinternals/\nExtract the downloaded zip contents to: C:\\Program Files (x86)\\SYSINT\nExtracting to this directory is useful for white-listing\nRun the program \u0026ldquo;sysdm.cpl\u0026rdquo; from the Windows menu\nClick the \u0026ldquo;Advanced Tab\u0026rdquo; -\u0026gt; Click \u0026ldquo;Environment Variables\u0026rdquo;\nDouble click on the \u0026ldquo;Path\u0026rdquo; variable -\u0026gt; Add a new variable pointing to: C:\\Program Files (x86)\\SYSINT\nNow we can run the Sysinternal tools from cmd without needing to be in the directory Add a new System variable named \u0026ldquo;_NT_SYMBOL_PATH\u0026rdquo; with the value \u0026ldquo;srvc:\\symbolshttp://msdl.microsoft.com/download/symbols\"\nTip Use the flag \u0026ldquo;/accepteula\u0026rdquo; to prevent the interactive window appearing when using the tools in a batch, for example.\n","date":"12 December 2019","externalUrl":null,"permalink":"/posts/setting-up-sysinternals/","section":"Blog Posts","summary":"A quick guide on installing Sysinternals in a permanent, system‑wide location and adding it to the PATH—along with setting the symbol path—so the tools can be run easily from any command prompt.","title":"Setting up Sysinternals","type":"posts"},{"content":"","date":"12 December 2019","externalUrl":null,"permalink":"/tags/sysinternals/","section":"Tags","summary":"","title":"Sysinternals","type":"tags"},{"content":"","date":"30 November 2019","externalUrl":null,"permalink":"/tags/http-header/","section":"Tags","summary":"","title":"HTTP Header","type":"tags"},{"content":"","date":"30 November 2019","externalUrl":null,"permalink":"/tags/iis/","section":"Tags","summary":"","title":"IIS","type":"tags"},{"content":"You may want to remove the server version from your HTTP response for security reasons, there are a couple ways you can go about this depending on how the header is being handled.\nMicrosoft-HTTPAPI/2.0 # If you are seeing the Server header Microsoft-HTTPAPI/2.0 then the header is being managed by HTTP.SYS driver and is not being forwarded to User mode for forwarding. To stripe this header you will need to:\nOpen REGEDIT and navigate to: HKLM\\SYSTEM\\CurrentControlSet\\Services\\HTTP\\Parameters Create a DWORD entry called DisableServerHeader in the following Registry key and set the value to 1. Open CMD as admin and restart the HTTP service with: net stop http net start http While in CMD restart IIS with: iisreset Microsoft-IIS/x.x # Install URL Rewrite on the IIS server. http://www.iis.net/downloads/microsoft/url-rewrite\nTip You can limit the exposure of the server header per site or globally\nSelect the site in particular when using URL Rewrite or select the IIS server to apply the changes globally\nClick on the View Server Variables in the Actions pane in the right hand side\nClick on the Add button\nEnter RESPONSE_SERVER in the textbox provided\nClick View Rules then add a new Outbound rule\nCreate an Outbound rule with the following:\n**Name: **The name for your rule Change **Matching Scope **to Server Variable Enter the pattern .+ Tip You can define the returned header in the Action Properties -\u0026gt; Value text box\nFinal Thoughts # You can either use this method to completly remove the Server headers from the site(s) or as an opportunity for misdirection. Its quite a cheap and easy change to implement, below is the output from a Telnet response which indicates the Server header has been stripped.\n","date":"30 November 2019","externalUrl":null,"permalink":"/posts/iis-server-header-hardening/","section":"Blog Posts","summary":"A short guide on removing or rewriting IIS and HTTP.SYS server headers to reduce information exposure in HTTP responses, using either a registry change for HTTPAPI or URL Rewrite rules in IIS to control what the Server header reveals.","title":"IIS Server Header Hardening","type":"posts"},{"content":"","date":"30 November 2019","externalUrl":null,"permalink":"/tags/server-header/","section":"Tags","summary":"","title":"Server Header","type":"tags"},{"content":"","date":"29 November 2019","externalUrl":null,"permalink":"/tags/how-to/","section":"Tags","summary":"","title":"How-To","type":"tags"},{"content":"","date":"29 November 2019","externalUrl":null,"permalink":"/tags/install/","section":"Tags","summary":"","title":"Install","type":"tags"},{"content":"","date":"29 November 2019","externalUrl":null,"permalink":"/tags/kali/","section":"Tags","summary":"","title":"Kali","type":"tags"},{"content":"","date":"29 November 2019","externalUrl":null,"permalink":"/tags/non-root/","section":"Tags","summary":"","title":"Non-Root","type":"tags"},{"content":"TOR shouldn\u0026rsquo;t be run as root. Many guides gloss over this by removing the root check in the start-tor-browser.desktop file, here is the better way:\nCreate a new user: adduser --home-dir /home/kali kali Download and extract TOR from https://www.torproject.org/projects/torbrowser.html.en Add the newly created user to the xhost file: xhost si:localuser:kali Copy the extracted TOR files to the home directory for the new user Change the permissions for the TOR files: sudo chown -R /home/kali Run TOR with the command: sudo -u kali -H /home/kali/start-tor-browser.desktop Verify TOR is being run with the new user account by running: ps aux | grep /tor Now TOR can be run by an account without root permissions\n","date":"29 November 2019","externalUrl":null,"permalink":"/posts/running-tor-on-kali-the-proper-way/","section":"Blog Posts","summary":"A short guide on running the Tor Browser safely by creating a non‑root user and launching Tor under that account, ensuring it never runs with elevated privileges.","title":"Running TOR on Kali | The proper way","type":"posts"},{"content":"","date":"29 November 2019","externalUrl":null,"permalink":"/tags/tor/","section":"Tags","summary":"","title":"TOR","type":"tags"},{"content":"","date":"28 November 2019","externalUrl":null,"permalink":"/tags/openvpn/","section":"Tags","summary":"","title":"OpenVPN","type":"tags"},{"content":"","date":"28 November 2019","externalUrl":null,"permalink":"/tags/set-up/","section":"Tags","summary":"","title":"Set-Up","type":"tags"},{"content":"Here is how to set-up OpenVPN on Kali. The process is (unsurprisingly the same for Debian), steps below:\nObtain OpenVPN certificates, key and openvpn.ovpn files from the provider\nca.crt: This is the certificate of the certification authority client.crt: This is the user certification file client.key: This is your private key file openvpn.ovpn: This is your OpenVPN configuration file Rename the openvpn.ovpn config file to something identifiable for the server being used (Server.conf in this example) Copy the downloaded files to the /etc/openvpn/ directory\nsudo cp Server.conf /etc/openvpn/ sudo cp ca.cert /etc/openvpn/ sudo cp client.crt /etc/openvpn/ sudo cp client.key /etc/openvpn/ Update the packages on Kali\nsudo apt-get update Ensure OpenVPN is installed with the required packages\nsudo apt-get install openvpn openssl openresolv Change directory to /etc/openvpn/\ncd /etc/openvpn Create a new file for the credentials to access the VPN\nsudo nano user.txt Enter the credentials in the following format:\nUsername Password Open the Server.conf file and make the following changes:\nChange auth-user-pass -\u0026gt; auth-user-pass /etc/openvpn/user.txt Under comp-lzo add the lines\nup /etc/openvpn/update-resolv-conf down /etc/openvpn/update-resolv-conf Now we need to point OpenVPN to this config file\nsudo nano /etc/default/openvpn Add the line AUTOSTART=\u0026quot;Server\u0026quot; (without the .conf) Ensure the DNS resolver is being pointed to your gateway\nRun cat /etc/resolv.conf If needed modify the file Run the command: sudo update-rc.d openvpn enable Now we can start the service with: sudo service openvpn start Check the service is running by running: systemctl status openvpn@server\n\u0026ldquo;Server\u0026rdquo; being the name of the .conf file being used If successful the output should look similar to:\nopenvpn@server.service - OpenVPN connection to server Loaded: loaded (/lib/systemd/system/openvpn@.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2019-11-27 00:00:00 GMT; 1h 00min ago Docs: man:openvpn(8) Tip Check for DNS leaks by visiting: https://dnsleaktest.com/\n","date":"28 November 2019","externalUrl":null,"permalink":"/posts/2019-11-28-setting-up-openvpn-on-kali/","section":"Blog Posts","summary":"A straightforward walkthrough for configuring OpenVPN on Kali or Debian by placing the provider’s certificates and config files in /etc/openvpn, setting up credential handling, enabling DNS updates, and configuring the service to auto‑start so the VPN connects reliably at boot.","title":"Setting up OpenVPN on Kali","type":"posts"},{"content":"","date":"28 November 2019","externalUrl":null,"permalink":"/tags/vpn/","section":"Tags","summary":"","title":"VPN","type":"tags"},{"content":"","date":"22 November 2019","externalUrl":null,"permalink":"/tags/api/","section":"Tags","summary":"","title":"API","type":"tags"},{"content":"This small project uses Troy Hunts\u0026rsquo; Have I Been Pwned fantastic (API) service along side a PS module which parses the JSON from the API. The purpose of this script is to read in emails addresses from file and then check them against HIBP to see if they are apart of any breaches or public pastes.\nYou will need a API key to make use of this API, but they are inexpensive. This script came about because I wanted to generate a custom report on a number of emails and because it was fun to make.\nThe script makes a number of CSVs which detail the breaches found and if there are any public pastes found.\n","date":"22 November 2019","externalUrl":null,"permalink":"/posts/finding-pwned-emails-with-hibp-and-powershell/","section":"Blog Posts","summary":"A small PowerShell project that reads email addresses from a file and checks them against the Have I Been Pwned API, generating CSV reports showing any breaches or public pastes associated with those accounts.","title":"Finding pwned emails with HIBP and Powershell","type":"posts"},{"content":"","date":"22 November 2019","externalUrl":null,"permalink":"/tags/hibp/","section":"Tags","summary":"","title":"HIBP","type":"tags"},{"content":"","date":"18 November 2019","externalUrl":null,"permalink":"/tags/minecraft/","section":"Tags","summary":"","title":"Minecraft","type":"tags"},{"content":"A fun micro-project, this Powershell script will select a random message of the day (MOTD) and write it to disk. Being an addition to the MC process monitoring script the scipt runs everytime the server restarts (Every 12 hours), also is engaging for the community to submit phrases and see their submissions displayed.\nBelow is the script, personally I placed it within a function and called it when required. Its pretty self-explanatory, I would like to mention more time was spent diagnosing why MC would have a fit and corrupt the file. Then I noticed the server.properties file was not being encoded in UTF-8, once amended it worked flawlessly.\nThe script also allows specific date overrides, which could be used for holidays and server related events.\nMOTD.txt is used to store the MOTD\u0026rsquo;s, which are separated by a new line.\nColourChart.txt contains the MOTD colours which are randomly selected and applied to the text.\n","date":"18 November 2019","externalUrl":null,"permalink":"/posts/minecraft-random-motd-selector/","section":"Blog Posts","summary":"A lightweight PowerShell micro‑project that randomly selects and writes a MOTD to disk at each server restart, supports date‑based overrides for special events, and uses simple text files for phrases and colour codes—making it a fun, community‑driven addition to your MC monitoring workflow.","title":"Minecraft Random MOTD Selector","type":"posts"},{"content":"I had a scenario in which a group of users needed to be added to a distribution group hosted on Exchange online.\nSo I thought Id made a little script which pulls users from the local Active Directory; checks them against the distribution group and if they are not present, adds them.\nYou may want to customize the \u0026ldquo;-SearchBase\u0026rdquo; parameter depending on your requirements. You could also pipe the ADUsers with \u0026ldquo;where\u0026rdquo; to get users with a specific property such as location.\n","date":"30 October 2019","externalUrl":null,"permalink":"/posts/finding-users-in-ad-and-adding-to-distribution-group/","section":"Blog Posts","summary":"A simple PowerShell script that pulls users from Active Directory, checks whether they’re already members of an Exchange Online distribution group, and adds any missing users—making it easy to keep cloud‑hosted groups aligned with on‑prem AD.","title":"Finding users in AD and adding to distribution group","type":"posts"},{"content":"","date":"27 October 2019","externalUrl":null,"permalink":"/tags/error/","section":"Tags","summary":"","title":"Error","type":"tags"},{"content":"","date":"27 October 2019","externalUrl":null,"permalink":"/tags/wid/","section":"Tags","summary":"","title":"WID","type":"tags"},{"content":"","date":"27 October 2019","externalUrl":null,"permalink":"/tags/wsus/","section":"Tags","summary":"","title":"WSUS","type":"tags"},{"content":"","date":"27 October 2019","externalUrl":null,"permalink":"/tags/wsus-maintainance/","section":"Tags","summary":"","title":"WSUS Maintainance","type":"tags"},{"content":"","date":"27 October 2019","externalUrl":null,"permalink":"/tags/wsus-server-cleanup/","section":"Tags","summary":"","title":"WSUS Server Cleanup","type":"tags"},{"content":"Whilst trying to cleanup and remove older Windows updates to free up space on my WSUS box I ran into the issue of WSUS crashing with the following exception:\nThe WSUS administration console was unable to connect to the WSUS Server Database. Verify that SQL server is running on the WSUS Server. If the problem persists, try restarting SQL.\nSystem.Data.SqlClient.SqlException -- Execution Timeout Expired. The timeout period elapsed prior to completion of the operation or the server is not responding. Source .Net SqlClient Data Provider Stack Trace: at System.Windows.Forms.Control.MarshaledInvoke(Control caller, Delegate method, Object[] args, Boolean synchronous) at System.Windows.Forms.Control.Invoke(Delegate method, Object[] args) at Microsoft.UpdateServices.UI.SnapIn.Wizards.ServerCleanup.ServerCleanupWizard.OnCleanupComplete(Object sender, PerformCleanupCompletedEventArgs e) Which from my understanding the query is taking too long to complete and the SQL server is simply timing out\u0026hellip; To correct this we need to connect to the WID and extend the timeout time.\nSteps to increase the query timeout time:\nConnect to the WID (Steps here) Right-Click the SQL instance and select properties Go to connections, then modify the timeout time to 1200 or greater (0 can prevent timeouts but is not recommended\n","date":"27 October 2019","externalUrl":null,"permalink":"/posts/wsus-server-cleanup-wizard-crashing-timing-out/","section":"Blog Posts","summary":"A quick note on resolving WSUS cleanup failures caused by long‑running SQL queries timing out; the fix involves connecting to the Windows Internal Database and increasing the query timeout value in the SQL instance properties so WSUS can complete its cleanup operations without crashing.","title":"WSUS Server Cleanup Wizard Crashing/Timing out","type":"posts"},{"content":"Whilst trying to figure out issues with a service which uses Windows Internal Database (WID) I came into the issue of actually connecting and managing it\u0026hellip;\nSteps to connect to WID:\n* Download and install Microsoft SQL Server Manager ([found here](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15)) onto the machine hosting the instance * Run Microsoft SQL Server Manager (SSMS) as **Administrator** (if not run as Administrator a permissions exception will be thrown when connecting) Enter the connection string, this will be different depending on the Windows version: * Windows 2003 - Windows 2008: \\\\.\\pipe\\MSSQL$MICROSOFT##SSEE\\sql\\query * Windows 2012 onward: \\\\.\\pipe\\MICROSOFT##WID\\tsql\\query * Connect using Windows Authentication ","date":"25 October 2019","externalUrl":null,"permalink":"/posts/connecting-to-windows-internal-database-wid/","section":"Blog Posts","summary":"A short guide on connecting to the Windows Internal Database by installing SSMS, running it as Administrator, and using the appropriate named‑pipe connection string for your Windows version so you can manage WID with standard SQL Server tools.","title":"Connecting to Windows Internal Database (WID)","type":"posts"},{"content":"","date":"25 October 2019","externalUrl":null,"permalink":"/tags/sql-server-management-studio/","section":"Tags","summary":"","title":"SQL Server Management Studio","type":"tags"},{"content":"","date":"25 October 2019","externalUrl":null,"permalink":"/tags/ssms/","section":"Tags","summary":"","title":"SSMS","type":"tags"},{"content":"","date":"25 October 2019","externalUrl":null,"permalink":"/tags/windows-internal-database/","section":"Tags","summary":"","title":"Windows Internal Database","type":"tags"},{"content":"","date":"25 October 2019","externalUrl":null,"permalink":"/tags/windows-server/","section":"Tags","summary":"","title":"Windows Server","type":"tags"},{"content":"","date":"12 October 2019","externalUrl":null,"permalink":"/tags/minecraft-powershell/","section":"Tags","summary":"","title":"Minecraft Powershell","type":"tags"},{"content":" About # The script was created to monitor a Minecraft server process on the host server (in this case Windows 2016). I also wanted the script to be capable of logging when the server started and stopped (intentionally or not). The script checks every 5 seconds to see whether Java is running, in the case Java is not running the script will call a function which attempts to start a Java process with the desired flags for the server (Such as RAM allocation\u0026hellip;).\nImprovements # A couple things could be improved; such as the ability to define flags for the server such as RAM allocation and other features relating to the Java instance. I also looked into the possibility of the script emailing upon a number of failed attempts, but I decided to use a third party tool to monitor the server port remotely which will email myself in the event of inaccessibility (This works better as the server can be tested from nodes across the internet and allows for a more inclusive test, such as DNS issues and other networking problems).\nI also run a MySQL instance in parallel with the game server for other plugins, so I may well integrate with that so I can conduct analysis of the servers restarting (time taken to startup and unexpected shutdowns for example).\nPersistence # To automate the starting of the game server you could create a batch wrapper which runs the PS1 file (as Windows will not run PS1 files directly through shortcuts), personally I use Scheduled Tasks through Windows Task Scheduler to start the server when the machine starts. Using Windows Task Scheduler in conjunction with AutoLogon can assist to create persistence in the event the virtual machine powers down, AutoLogon allows you to securely define an account for the machine to sign into on automatically startup (I would strongly recommend a service account used only for this purpose).\nPlugin Considerations # I currently use a plugin on the game server to send a stop command to conduct scheduled restarts (through the use of this script to bring to server up), this is useful to keep the Ticks-Per Second (TPS) of the server high and reduce lag. There are plugins which can be used to create a CRON job to automate this but your choice will ultimately depend on the server type and the game version.\n","date":"12 October 2019","externalUrl":null,"permalink":"/posts/minecraft-server-process-monitoring-powershell/","section":"Blog Posts","summary":"A PowerShell script designed to monitor and automatically restart a Minecraft server on Windows by checking for the Java process every few seconds, logging start/stop events, and providing persistence through scheduled tasks. The setup can be extended with custom server flags, external monitoring tools, and optional MySQL integration for deeper analysis of uptime and restart behaviour.","title":"Minecraft Server Process Monitoring - PowerShell","type":"posts"},{"content":"","date":"12 October 2019","externalUrl":null,"permalink":"/tags/minecraft-start-script/","section":"Tags","summary":"","title":"Minecraft Start Script","type":"tags"},{"content":"","date":"10 October 2019","externalUrl":null,"permalink":"/tags/archiving-logs/","section":"Tags","summary":"","title":"Archiving Logs","type":"tags"},{"content":"The purpose of this PowerShell script was to automate the cleaning of logs files by zipping them and shipping them to another folder or drive, but to also give the option to delete the source files after being processed.\nThe script has the option to group the files by the day they were written or to archive them by individual file, which can be useful if the source of the logs is particularly spammy.\nExample # I noticed my webserver had been filling up over the last few months, so for the purpose of an example of this scripts usefulness I put this script to use (despite IIS having the functionality to compress folders, but I needed an example)\nBefore # After # Command used # ","date":"10 October 2019","externalUrl":null,"permalink":"/posts/automating-log-file-archiving-powershell/","section":"Blog Posts","summary":"A PowerShell utility that automates log maintenance by compressing log files, optionally deleting the originals, and supporting both per‑file and per‑day grouping. It’s a practical way to reclaim disk space on busy servers, especially when dealing with large or noisy log directories.","title":"Automating Log File Archiving - PowerShell","type":"posts"},{"content":"","date":"10 October 2019","externalUrl":null,"permalink":"/tags/file-management/","section":"Tags","summary":"","title":"File Management","type":"tags"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/hallgate/","section":"Tags","summary":"","title":"Hallgate","type":"tags"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/hallgate-timber-website/","section":"Tags","summary":"","title":"Hallgate-Timber Website","type":"tags"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/joshua/","section":"Tags","summary":"","title":"Joshua","type":"tags"},{"content":"This project is a follow up from my previous post \u0026ldquo;Hallgate Timber Website Remodel\u0026rdquo;. Whilst my previous website design met the criteria put forward to myself, requirements had changed and to accommodate the changing criteria another website overhaul was needed. A number of new requirements included; Dynamic menu, Website contact form, SEO overhaul and most importantly the website needed to be Mobile friendly.\nThe new website design criteria explained # Dynamic Menu # A dynamic menu was a suggestion put forward when I began designing my first remodel, in hind sight it was more important than I believed at the time. In my previous design users navigated the website via categories e.g. A user wants to view Sheds, from the homepage a typical user action would be Homepage \u0026gt; Garden Buildings \u0026gt; Sheds. Whilst this wasn\u0026rsquo;t optimal, it worked and I was able to add more structure to the website. The problems with having a static menu became obvious quite soon after publishing the new site, more user steps means more bandwidth usage and some customers had difficulty traversing the website.\nWebsite Contact Form # The original website utilized a contact form generated by our CMS (Content management system), which did function as intended. However, to access the contact form users had to click through a link embedded on the contact page. This was improved by creating and embedding a contact form within the contact us page, the new contact form provides users a more seamless experience when attempting to make contact or request information. The contact form does offer SQL injection protection, basic spam checks through a CAPTCHA as well as logging of IP addresses so that repeat offenders can be blacklisted. The way customer query\u0026rsquo;s are handled has also been changed, previously the contact form emailed directly to the managers email only which was only accessible through his computer. Now all customer query\u0026rsquo;s go to shared email address which is synchronized to each sales team members computer, allowing for increased productivity and quicker turn around time on customer query\u0026rsquo;s.\nSEO Overhaul # Not a lot of consideration was put into my previous website design, that was mainly due to the site being a placeholder and my knowledge on SEO needing a brush up. Through the use of online resources and my managers guidance, I was able to improve SEO on each page. I used multiple online SEO testers and checkers to find errors to be corrected, whilst it was not a perfect methodology (as many sites conflict on SEO guidance) it certainly has been much improved.\nMobile Compatibility # Mobile compatibility was a major caveat for SEO as well as user experience, with mobile users outweighing conventional computer users it certainly was an insure which couldn\u0026rsquo;t be ignored. The new website design needed to dynamic and accommodate as many users as possible. The website needed to present its content and menu system to differing resolutions, this was achieved through a viewport and defining how to display the content/menu on different resolutions. After a fair amount of research into responsive web design and testing, I can say I am satisfied with the results.\n","date":"9 October 2019","externalUrl":null,"permalink":"/posts/mobile-friendly-website-design-hallgate/","section":"Blog Posts","summary":"A follow‑up web‑development project that rebuilds the Hallgate Timber website to meet new requirements, including a dynamic navigation menu, an embedded contact form with improved handling and security, an SEO overhaul, and full mobile‑friendly responsiveness. The redesign focuses on smoother user journeys, better accessibility for staff handling enquiries, and modern responsive design practices to support the growing number of mobile visitors.","title":"Mobile Friendly Website Design - Hallgate Timber","type":"posts"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/robbins/","section":"Tags","summary":"","title":"Robbins","type":"tags"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/timber/","section":"Tags","summary":"","title":"Timber","type":"tags"},{"content":"","date":"9 October 2019","externalUrl":null,"permalink":"/tags/website/","section":"Tags","summary":"","title":"Website","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/1nf/","section":"Tags","summary":"","title":"1NF","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/2nf/","section":"Tags","summary":"","title":"2NF","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/3nf/","section":"Tags","summary":"","title":"3NF","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/candidate-key/","section":"Tags","summary":"","title":"Candidate Key","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/categories/coursework/","section":"Categories","summary":"","title":"Coursework","type":"categories"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/data/","section":"Tags","summary":"","title":"Data","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/data-flow-diagram/","section":"Tags","summary":"","title":"Data Flow Diagram","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/categories/database-design/","section":"Categories","summary":"","title":"Database Design","type":"categories"},{"content":"Database Normalisation and informal design guidelines follows on from \u0026ldquo;Database planning of modules and mechanisms\u0026rdquo;, this time I am required to apply Normalisation upon my database design, discuss the Four informal design guidelines that may be used as measures to determine the quality of relation schema design as well as show sample SQL statements.\nThe Four informal design guidelines that may be used as measures to determine the quality of relation schema design # Guideline 1: Each record in a relation should represent one entity or relationship instance # This guideline is to design a relational schema so that its meaning is easily explained. Another goal of this guideline is to not combine attributes from multiple relationships and entity types into a single relation. Information should not be stored redundantly therefore preventing storage being wasted, only foreign keys should be used when referring to other entities.\nGuideline 2: No insertion, deletion, or modification anomalies are present in the relations # This guideline is to help design the database relation schema so that there are no insertion, deletion or modification anomalies present in the relations, improper groupings of attributes into a relation schema will result in wasted storage, insert anomalies, delete anomalies and modification anomalies.\nGuideline 3: Relations should be designed such that their records will have as few NULL values as possible # This guideline is to help prevent placing attributes in a database relation which value may frequently be null, however, if nulls are unavoidable they should apply in exceptional cases only and not the greater part of values in a relation. Problems with having null values are as follows: wasting of storage space, Problems with understanding the meaning of attributes, Problems with applying certain aggregate function and also problems with JOIN operations.\nAs a result of applying normalisation the schema has got minimal attributes which may be null, the only attribute that may be null is the Discount attribute within the table VehicleRental.\nGuideline 4: Design relation schemas so that they can be joined with equality conditions on attributes that are either primary keys or foreign keys in a way that guarantees that no spurious records are generated # This guideline is to help design a relational schema so that it can be joined with equal conditions of attributes that are either primary keys or foreign keys in ways that will guarantee that no spurious records are generated. To stop generations spurious records, relations should no contain matching attributes other than the foreign/primary key relation, however, if such relations are unavoidable they should not be joined on such attributes because the join may create spurious records.\nKey attributes for Renting Vehicle relation and its functional dependencies # In this task key attributes (candidate keys, primary keys, prime attributes and non-prime attributes) need to be identified within the RentingVehicle relation. Below is the RentingVehicle relation:\nRentingVehicle: {Customer-Firstname, Customer-Lastname, Customer-Email, RentDate, Authorised-by, Vehicle-Model, Vehicle-Colour, Vehicle-Type, VehicleRegistration-No, Discount, Renting-Price, Pickup-Location, Drop-off-Location, Pickup-Date/time, Drop-off-Date/time}.\nCandidate keys:\n{VehicleRegistration-No} {Customer-Email} {RentDate, VehicleRegistration-No} {RentDate, VehicleRegistration-No, Customer-Email} Primary key:\n{RentDate, VehicleRegistration-No, Customer-Email} Prime attributes:\n{RentDate, VehicleRegistration-No, Customer-Email} Non-prime attributes:\n{Customer-Firstname, Customer-Lastname, Authorised-by, Vehicle-Model, Vehicle-Colour, Vehicle-Type, Discount, Renting-Price, Pickup-Location, Drop-off-Location, Pickup-Date/time, Drop-off-Date/time} Functional dependencies:\n{VehicleRegistration-No -\u0026gt; Vehicle-Model} {VehicleRegistration-No -\u0026gt; Vehicle-Colour} {VehicleRegistration-No -\u0026gt; Vehicle-Type} {Customer-Email -\u0026gt; Customer-Firstname} {Customer-Email -\u0026gt; Customer-Lastname} The normalisation process for the RentingVehicle relation # Normalization is a technique in which data is organised in the database, normalisation is an approach in which tables are decomposed to prevent data redundancy and unwanted characteristics such as Insertion, Modification and Deletion anomalies.\nWithout applying normalisation, handling and updating the database will become harder, and may result in loss of data, insertion anomalies, deletion anomalies, and modification anomalies.\nUn-Normalised Form – UNF # Below is the data in UNF, UNF is the preparatory stage of normalisation as each stage is dependant upon the previous.\nFirst Normal Form - 1NF # In First Normal Form (1NF) there cannot be any rows of data in which repeating groups of information can be stored, each column must contain unique data. Each table should be organised into rows, with a primary key which makes it unique.\nAbove are the 2 tables which have been generated by applying 1NF, because the customer can rent upto 5 vehicles at a time, so a separate table has been generated to contain all the vehicles which can be rented. The primary key for the first table is made up of {Customer-Email, RentID, VehicleRegistration-No} as that is unique. To ensure all data is atomic Pickup-Location has been broken down into {Pickup-Address, Pickup-Town, Pickup-Postcode} and Drop-off-Loction has been broken down into {Drop-off-Address, Drop-off-Town, Drop-off-Postcode}\nSecond Normal Form - 2NF # In Second Normal Form (2NF) there cannot be any partial dependency of any of the columns on a primary key, meaning all non-key attributes of the table must depend on all parts of the primary key.\nBy applying 2NF there are now 4 Tables: CustomerDetails, VehicleRental, Vehicle and RentDetails.\nThird Normal Form - 3NF # In Third Normal Form (3NF) every non-prime attribute of the table must be dependent on the primary key (just like in 2NF) but also any transitive functional dependency should be removed from each table. The purpose of removing transitive dependency is that data duplication is reduced and data integrity will be achieved.\nBy applying 3NF, 2 new tables have been created Drop-off and Pickup, these have been created to remove transitive dependency\nSQL statements for defining the database solution # Below are the SQL statements for defining the database solution\nCreate “Lincoln Vehicle Rent\u0026quot; database. CREATE SCHEMA LincolnVehicleRent AUTHORIZATION A1; Create your 2 tables from your design, including data types, integrity constrains of relationships between tables; with enforcing integrity on each relationships. Change the definition of one of your tables by adding at least one attribute constraint and one table constraint. Granting account A1 the privilege to insert and delete tuples in vehicle relation with the ability to propagate these privileges to additional accounts Revoking account A1 the privilege granted to insert tuples in customer relation. SQL statements for manipulating the database # Inserting a valid record for the customer table; deleting records for the customer table; editing records in the customer table List of the most favourite vehicle-make for customers. ","date":"23 August 2016","externalUrl":null,"permalink":"/posts/database-normalisation-informal-guidelines/","section":"Blog Posts","summary":"A coursework project demonstrating the application of database normalisation—from UNF through 1NF, 2NF, and 3NF—using a vehicle‑rental scenario. It explains the four informal design guidelines for evaluating relation quality, identifies keys and functional dependencies, shows the step‑by‑step decomposition of the RentingVehicle relation, and provides example SQL statements for defining, modifying, and manipulating the resulting database schema.","title":"Database normalisation and informal guidelines","type":"posts"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/design/","section":"Tags","summary":"","title":"Design","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/diagram/","section":"Tags","summary":"","title":"Diagram","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/foreign-key/","section":"Tags","summary":"","title":"Foreign Key","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/informal-guidelines/","section":"Tags","summary":"","title":"Informal Guidelines","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/normalisation/","section":"Tags","summary":"","title":"Normalisation","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/primary-key/","section":"Tags","summary":"","title":"Primary Key","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/schema/","section":"Tags","summary":"","title":"Schema","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/sql/","section":"Tags","summary":"","title":"SQL","type":"tags"},{"content":"","date":"23 August 2016","externalUrl":null,"permalink":"/tags/unf/","section":"Tags","summary":"","title":"UNF","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/context-level-dfd/","section":"Tags","summary":"","title":"Context Level DFD","type":"tags"},{"content":"You have been contracted to design a database for a car rental company called \u0026ldquo;Lincoln Vehicle Rent\u0026rdquo; based in Lincolnshire. The company needs to provide full rental service which includes keeping track of customers, cars, orders and staff. Your database solution should offer the opportunity for customers to specify particular parameters about what they are looking to rent. They should have the possibility to rent one or multiple cars for the same customer; a customer might be a company or an institution, but you might have a cap on the number of rented vehicles. A customer should have the ability to pick up and drop off the vehicle in different locations.\nAppraise and analyse the scenario # Having read the scenario I have been contracted to design a system for a car rental company called “Lincoln Vehicle Rent” which is based in Lincolnshire. The company aims to provide a full rental service which includes tracking its customers, cars, orders and staff. The database needs to allow the customer to set parameters to find the type of vehicle they wish to rent. The customer should be able to rental multiple vehicles, but also may be limited to how many vehicles they may be able to rent at any given time. The customer also needs to be able to decide where the vehicles are picked up and dropped off.\nThe system needs to apply restriction or conditions based on their vehicles and what driving licence categories the customer is entitled to drive, insurance policy and the customer’s age. The system will need to be able to check the values the customer enters against what vehicle the customer wants to rent, to ensure they qualify to rent their selected vehicle.\nThe system will need to be able to track the status of the company’s vehicles to determine what state they are in, whether it may be rented, being serviced, cleaned, sold, scarped or even stolen. So the system will need to have records of all vehicles they own, information regarding the vehicles and including their current state.\nThe system needs to be able to store details regarding inspection of the vehicle, this may include the member of staff who made the inspection, was the customer present during the inspection, details of where there are dents and scratches etc. on the vehicle. These details need to be noted on the rental agreement. This process will need to be repeated when the customer drops off the vehicle so that if any new dents, scratches etc. are present on the vehicle the customer will be held liable for the repair costs.\nThe system needs to be able to record feedback from the customers for the purpose of marketing and enhancing their services in addition to their complaints, the system should also be able to store the level of demand for their vehicles based on colour, make and size. The system could use this system for built in analytics such as graph’s based on the most popular vehicle, most popular colour vehicle rented etc. so that this information can be easily used by the company for their future growth plan.\nThe system needs to allow staff to process the renting and checking in and out of the vehicle selected for rental, the system needs to be able to display the status of their vehicles and allow them to manage the stock of their vehicles.\nWhat information the customer should provide to rent a vehicle # Pick up and drop off location Date in which they want to rent the vehicle as well as duration Vehicle type/class Date of birth Driving License Category Type(s) What information Lincoln Vehicle Rent should provide about the vehicle # Engine size Whether its automatic or manual Fuel type Number of seats Mileage of the vehicle Car model/type Vehicle class What information Lincoln Vehicle Rent needs to know from the customer to complete the renting process # Full Name\nContact details i.e. Contact numbers and current address\nPayment details\nDriving License\nPick up and drop off locations\nIdentifying brief entities (nouns), attributes and relationships (verbs). # Scenario: You have been contracted to design a database for a car rental company called “Lincoln Vehicle Rent” based in Lincolnshire.\nThe company needs to provide full rental service which includes keeping track of customers, cars, orders and staff. Your database solution should offer the opportunity for customers to specify particular parameters about what they are looking to rent. They should have the possibility to rent one or multiple cars for the same customer; a customer might be a company or an institution, but you might have a cap on the number of rented vehicles. A customer should have the ability to pick up and drop off the vehicle in different locations.\nThe company should have some conditions/ restrictions based on vehicles and customer driving licence, insurance policy and customer age.\nSomething you need to consider when you calculate the rental price: offering discount, exceeding limited mileage, weekend rate and travelling into another country.\nThe company needs to track the status of their vehicles as they might be rented, under service, cleaning, sold, scraped, stolen, etc.\nA member of staff with the customer will inspect the vehicle carefully on collection and any scratches, dings, dents or scuffs will be noted on the rental agreement. This inspection will be carried out again when the customer returns the vehicle and the customer will be liable for any repair or refurbishment costs in the case of damage or vandalism.\nThe company needs to record some feedback and rating from their customers for the purpose of marketing and enhancing their services in addition to their complaints. Also they are interested to know the level of demand on their vehicles based on colour, make, size, etc.; as that will be used for their future growth plan.\nStaff should be able to process the renting and checking in and out of vehicle. Also they should maintain the status of their vehicles and manage their stock of vehicles.\nFunctional Requirements # Reservation # The system must allow the customer/institution to register for reservation\nThe system shall allow the customer to view details of selected vehicle\nThe system must notify if the selected vehicle is unavailable\nThe system shall allow the customer to rent more than one vehicle\nThe system must allow the customer to specify particular parameters about what they are looking to rent\nThe system must notify the customer if they have exceeded the maximum number of vehicles they may rent\nThe system must notify if customer cannot rent a certain vehicle because of their age, driving license entitlements and insurance policy\nThe system must list all available vehicles\nThe system shall allow the customer to cancel their reservation using their reservation-ID\nThe system shall allow the customer to state pick up and drop off locations\nVehicle # The system must allow staff to register new vehicles\nThe system shall allow customers to view vehicles on the list\nThe system shall allow staff to view vehicles on the list\nThe system shall allow staff to search for vehicles by reference\nThe system shall allow customers to search for vehicles by reference\nThe system shall allow staff to view a list of available vehicles\nThe system shall allow customers to view a list of available vehicles\nThe system shall staff to view a list of unavailable vehicles\nThe system shall allow staff to update the condition of the vehicle (scratches, dents, etc.)\nThe system shall allow staff to update the state of the vehicle (serviced, cleaned, sold, scarped or stolen)\nRental # The system shall allow staff to enter customers onto the rental list\nThe system shall allow the staff to manage the customer rental records on the customer rental list\nThe system shall allow staff to apply discounts, apply weekend rates\nThe system shall manage whether the rental agreement allows travel out of the country\nThe system shall allow staff to see whether the limited mileage is exceeded\nThe system shall allow staff to see all customer rental records\nThe system shall allow staff to see which customers rent vehicles\nThe system shall allow staff to see which customers are currently renting a vehicle, the quantity of vehicles being rented and the cap\nFeedback # The system shall allow customers to enter feedback\nThe system shall record the customers vehicle information\nThe system shall allow staff to view feedback\nThe system shall allow staff to view analytics, such as graphs of demand of vehicle colour\nDataflow diagrams # Context Level DFD # Level-1 DFD # Initial conceptual design # Here I have provided a description on the entities with a preliminary design of the entity types and their relationships.\nEntity Name Definition\nStaff Is a strong entity is responsible for managing Vehicles, Rental Agreements and Reservation information, and needs to store staff details\nCustomer Is a strong entity makes reservations on vehicles and stores customer details\nReservation Is a strong entity checks whether a vehicle is available for rent and stores reservation applications\nRent Is a strong entity is where rental agreements are made and stored\nVehicle Is a strong entity is where the vehicle details. state and condition are stored\nFeedback Is a weak entity which relies on the customer. feedback from the customer is stored and processed into analytics such as graphs for the staff\nPreliminary Design of Entity Types # Entity Name Attributes\nStaff (empID, name, address, telNo, NI\nCustomer (customerID, name, address, telNo, DOB, paymentInfo)\nReservation (resID, Res_date, pickupDate, returnDate, pickupLoc, returnLoc, regNo, customerID)\nRent (rentID, rentDate, returnDate, dailyRentFee, totalRentFee, fuelCharge, damages, discount, weekendFee, abroadFee, pickupLoc, returnLoc, regNo, empNo, customerID, resID)\nVehicle (regNo, manufacture, type, model, noOfSeat, fuelType, condition, status, dailyPrice, colour, empID)\nFeedback (feedbackID, feedback, customerID)\nPreliminary Design of Relationship Types # A staff member approves a many rental agreements\nStaff (1:M) Approve Some of the staff are participating in this relationship All of the rental agreements are participating in this relationship A staff member adds/ updates many vehicle records\nStaff (1:M) manages Some of the staff are participating in this relationship All of the reservations are participating in this relationship A Customer makes many Reservations\nCustomer (1:M) applies for Some of the customers are participating in this relationship All of the vehicles are participating in this relationship A Customer may agree-to many rental agreements\nCustomer (1:M) agrees-to Some of the customers are participating in this relationship All of the Rental agreements’ are participating in this relationship A Reservation may be for more than one vehicle\nReservation (1:M) involves Some of the reservations are participating in this relationship All of the vehicles are participating in this relationship A Rental Agreement has one reservation\nRental Agreement (1:1) contains Some of the rental agreements are participating in this relationship Some of the reservations are participating in this relationship A Rental Agreement may have many vehicles\nRental Agreement (1:M) involves Some of the rental agreements are participating in this relationship All of the reservations are participating in this relationship A customer gives write more than one feedback\nCustomer (1:M) writes Some of the customers are participating in this relationship All of the feedback are participating in this relationship Entity-Relational (ER) model # Mapping the ER Model # Step 1: Mapping of Regular Entity Types\nEntity Name Attributes\nStaff (empID, NI\nCustomer (customerID, DOB)\nReservation (resID, resdate, pickupDate, returnDate, pickupLoc, returnLoc)\nRent (rentID, dailyRentFee, totalRentFee, fuelCharge, damages, discount, weekendFee, abroadFee)\nVehicle (regNo, manufacture, type, model, dailyPrice)\nFeedback (feedbackID, feedback)\nStep 2: Mapping of Weak Entity Types\nFeedback (feedbackID, feedback, customerID)\nStep 3: Mapping of Binary 1:1 Relationship Types\nReservation (rentID, dailyRentFee, totalRentFee, fuelCharge, damages, discount, weekendFee, abroadFee, resID)\nStep 4: Mapping of Binary 1:N Relationship Types.\nReservation (resID, resdate, pickupDate, returnDate, pickupLoc, returnLoc, regNo, customerID)\nRental (rentID, pickupDate, returnDate, dailyRentFee, totalRentFee, fuelCharge, damages, discount, weekendFee, abroadFee, pickupLoc, returnLoc, regNo, empNo, customerID, resID)\nVehicle (regNo, manufacture, type, model, fuelType, condition, status, dailyPrice, colour, empID)\nStep 5: Mapping of Multi-valued attributes.\nVehicleSize regNo, noOfSeat\nOutput of the mapping process # ","date":"16 July 2016","externalUrl":null,"permalink":"/posts/database-planning-modules-mechanisms/","section":"Blog Posts","summary":"A full database‑design project for Lincoln Vehicle Rent, covering requirement analysis, functional specifications, DFDs, an ER model, and the mapping of that model into a relational schema. It defines the key entities—customers, staff, vehicles, reservations, rentals, and feedback—and shows how they interact to support vehicle availability, rental processing, inspections, restrictions, and customer feedback within a complete, normalised database structure.","title":"Database planning of modules and mechanisms","type":"posts"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/dfd/","section":"Tags","summary":"","title":"Dfd","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/entity-relationship-diagram/","section":"Tags","summary":"","title":"Entity Relationship Diagram","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/erd/","section":"Tags","summary":"","title":"ERD","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/flow/","section":"Tags","summary":"","title":"Flow","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/initial-conceptual-design/","section":"Tags","summary":"","title":"Initial Conceptual Design","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/level-1-dfd/","section":"Tags","summary":"","title":"Level 1 DFD","type":"tags"},{"content":"","date":"16 July 2016","externalUrl":null,"permalink":"/tags/relationship/","section":"Tags","summary":"","title":"Relationship","type":"tags"},{"content":"Write a program in MATLAB that perform the following tasks for all the images:\nIdentify the type of noise in the image and seek a way to remove the noise. The program at this stage should generate a de-noised image. (Note that not all images have the same type of noise) Segment the simulated lung area from the image. The program should generate a binary image showing the simulated lung area on a black background (e.g. Figure 1 shows an example of the segmented lung region) Count and display in the Command Window the number of circles inside the lung region. For some images, half circles also appear at the edge of the lung region. Your program should count those as well. Here, your program should display a figure with detected circles marked (e.g. you may mark it with a small cross or a bounding box). Challenge task - for additional credit, make your program also perform the following: Segment detected circles from the lung region and draw their boundaries (use a green line). Task 1 # The noise on each image was initially detected by sight, images 1 through to 6 appeared to have contained salt-and-pepper noise, so I tried applying a median filter onto the images to reduce the noise. Firstly I implemented Matlab’s function “rgb2gray” (MathWorks, 2015), which converts an RGB image to an grayscale intensity image, by removing hue and saturation and keeping the images luminance information. The image was converted to grayscale as the image was mainly black and white with an exception to the red-cross which was not wanted and had to be later removed.\nAfter converting the image to grayscale the image was cropped using Matlab’s “imcrop” (MathWorks, 2015), the image was cropped to a fixed rectangle which kept the lung region across all images and removed unwanted data in the image.\nBy converting the image to grayscale the amount of matrices storing information was reduced to 1 instead of 3 (Red, Green, Blue) improving efficiency as many of the functions implemented loop through the image matrix. Once an image is converted to grayscale its colour information will be lost, although this is not important for this application as the only colour on the images was the unwanted red-cross.\nAfter the image is converted to grayscale it is passed through a median filter, the median filter works by taking the values of the input image corresponding to a desired sub-window or mask, I found a 3x3 mask worked best at producing a more clear filter for reducing noise, the values are then sorted and the value in the middle is then applied, for a 3x3 mask the middle value would be the 5th largest value as there are 9 values in the mask at any time.\nThe median filter removed most of the noise on the images except image ‘4.jpg’ which still had noise, so an Gaussian filter was applied on top of the median filter. The Gaussian filter smoothed the image, and removed the remaining noise. The Gaussian filter works by using a kernel to represent the Gaussian shape. σ determines the width of the filter which in turn determines the amount of smoothing on the image, I found the optimal value for me was 1.4, with a grid of 5x5.\nTask 2 # To segment the lungs region from the image after removing the noise, the image had to be converted to a binary image, this was achieved by comparing each value in the image to a threshold, depending on the images’ value it a value of 1 or 0 is assigned to a new matrix the size of the inputted image, which after all the values have been compared and assigned a 1 or a 0 is the new outputted image.\nFrom adjusting the threshold with the different images I concluded that the threshold of 89 worked best for me, so anything less than 89 it is assigned a 1 making it white, if the value being compare is above 89 the value is assigned 0 making it black.\nIt is at this stage Opening (Erosion followed by Dilation) is performed on the image. Opening is applied to smooth foreground objects by breaking narrow links and remove thin protrusions, this will help later on in the program when identifying circles. Opening was used instead of Closing (Dilation followed by Erosion) as the images foreground was white, Closing smooths with respect to the background (black in this case) and as I want to make the circles more defined Opening was implemented.\nTo remove the background Connected Components was used to identify each component in the image and label is respectively, once the background was identified using its label it was assigned the value of 0 making it black. The lungs were then identified, labelled and assigned the value of 1 making them white. By adding the labels of the lungs onto the background an image is produced highlighting only the lungs and removing the white background.\nConnected Components works by identifying sets of values (1, 0) by checking if values of the same are connected, if sets of values meet the criteria they are labelled as a group and can be manipulated accordingly.\nBy applying connected components some of the unwanted background features are also removed as seen in Figure 2, below:\nTask 3/4 # To achieve task 3 and task 4 I implemented Matlab’s function “imreadcircles” (Mathworks, 2015), “imreadcircles” use a Circle Hough transform.\nThe Circle Hough Transform aims the find circular patterns with a given radius within an image, the Circle Hough transform does its detections by a technique which filters the image for matching circles. The Circle Hough Transform is useful as it can operate with good results on images with noise, it can detect circles which are partially obstructed (such as circles on the wall of the lung) as well as working well with low contrast objects (Ioannou, et al., 1999). As imreadcircles uses a Circle Hough Transform it makes it ideal for this task of finding tumours in the lung and tumours which are partially obscured on the lung wall.\nThe parameters which are passed through the “imreadcircles” in the program are identified and described as follows:\nRmin, Rmax; Rmin is the minimum radius for the circle in which to detect, while Rmax is the maximum radius for the circle to detect.\nObjectPolarity: defines the polarity (bright or dark) or the circular objects, bright identifies circles which are brighter than the background and darker identifies circles which are darker than the background.\nSensitivity: defines the sensitivity of the Circle Hough Transform accumulator array, the higher the value the more sensitive the Circle Hough Transform accumulator array.\nEdgeThreshold: sets the gradient threshold for declaring edge pixels in an image.\nTo count and display the number of circles detected, the output variable “radiiBright “ of “imshowcircles” is stored and then displayed into the console.\nTo display a green circle around the identified circles the function “viscircles” (MathWorks, 2015), is called and using the outputted variables (“centersBright”, “radiiBright”) from “imshowcircles” they are plotted on top of the segmented lung region.\nReferences # Ioannou, D., Huda, W. \u0026amp; Laine, A., 1999. Circle recognition through a 2D Hough Transform and radius histogramming. *Image and Vision Computing, *Volume 17, pp. 15-28.\nMathWorks, 2015. *Convert RGB image or colormap to grayscale. *[Online] Available at: https://uk.mathworks.com/help/matlab/ref/rgb2gray.html [Accessed 26 Novemeber 2015].\nMathWorks, 2015. *Create circle. *[Online] Available at: https://uk.mathworks.com/help/images/ref/viscircles.html [Accessed 26 November 2015].\nMathWorks, 2015. *Crop image. *[Online] Available at: http://uk.mathworks.com/help/images/ref/imcrop.html [Accessed 26 November 2015].\nMathworks, 2015. *Find circles using circular Hough transform. *[Online] Available at: http://uk.mathworks.com/help/images/ref/imfindcircles.html [Accessed 26 November 2015].\n","date":"10 July 2016","externalUrl":null,"permalink":"/posts/digital-image-processing-matlab/","section":"Blog Posts","summary":"A MATLAB pipeline that denoises lung X‑ray images (median and Gaussian filtering), segments the lung region via thresholding, morphological opening, and connected components, then detects and counts circular lesions—including partial ones—using the circular Hough transform (imfindcircles). Detected circles are overlaid with green boundaries, and the total count is printed to the Command Window.","title":"Digital Image Processing - Matlab","type":"posts"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/categories/dissertation/","section":"Categories","summary":"","title":"Dissertation","type":"categories"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/dissertation/","section":"Tags","summary":"","title":"Dissertation","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/dissertation-project/","section":"Tags","summary":"","title":"Dissertation Project","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/image/","section":"Tags","summary":"","title":"Image","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/categories/image-processing/","section":"Categories","summary":"","title":"Image Processing","type":"categories"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/image-processing/","section":"Tags","summary":"","title":"Image Processing","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/joshua-robbins/","section":"Tags","summary":"","title":"Joshua Robbins","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/mathworks/","section":"Tags","summary":"","title":"MathWorks","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/matlab/","section":"Tags","summary":"","title":"MATLAB","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/pacs/","section":"Tags","summary":"","title":"PACS","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/pacs-network-opimisation/","section":"Tags","summary":"","title":"PACS Network Opimisation","type":"tags"},{"content":" Abstract # This project aims to address PACS network optimisation, as it is being increased in clinical use. More data is being transmitted across PACS network environments which may include patient records or patients’ medical images. Without optimisation the network may suffer performance issues. This project plans to design and implement different topologies for the PACS network through simulations created using NS2. From the simulations produced within NS2, meaningful data is extracted from the trace files which is then statistically analysed though graphs and computation of the data to discover an optimised network. The resulting graphs depict packet loss through the testing of incrementing packet throughput rates. The outcome of this analysis is the Mesh topology, which proves to be the most resilient topology. However, further research into combining topologies should be conducted. Please note this paper has been adjusted to accommodate itself as a blog entry.\nAcknowledgements # I would like to thank Dr Massoud Zolgharni for continuous support throughout the project, offering insightful guidance and feedback during each stage of the projects development.\n1. Introduction # A Picture Archive Communication System (PACS) is a system which is comprised of medical imaging scanners (for example: CT or MRI scanner), storage mechanisms, information communication technologies, displays, and clinical workflow. A PACS network is the combination of PACS systems and nodes (computers) accessing storage of images. At any time, image files (often many) can be requested for viewing or being saved to storage mechanisms.\nThe concept of digital communication and digital radiology was introduced in the late 1970s and early 1980s. The beginning of PACS was attributed to development in the USA, where research was contributed by laboratories within universities and small companies in the private industry which had entered the field in 1982. In the early years of PACS evolution, research and development varied in the region which it was conducted; PACS research in North America was largely supported by government agencies and manufacturers (Huang, 2011). Research in Europe was supported through multi-national efforts since PACS components weren’t as accessible in Europe as N.A. and Asia so research emphasized on modelling and simulation. PACS research in Asia was led by Japan which treated it as a national project and resources were distributed to many manufacturers and university hospitals (Lemke, 2011).\nA PACS network often has many workstations and modularity’s sending packets of information around in large numbers, being requested for viewing or being stored in PACS archives. (Santos, et al., 2015) Highlights this, although their research was oriented around DICOM metadata and there was a clear pattern of more images being produced year by year, all of which needs to be stored with redundancy in mind. With all this data being transmitted and received on the network, optimisation of how these nodes communicate would prove valuable for increased network performance.\nDue to large image files being requested or being stored, network bandwidth can often be limited which causes slowness of the network, slowing workflow and potentially causing a lower quality of service for patients. As a PACS network can often consist of multiple storage mechanism’s which allow for data redundancy, bandwidth can be further limited.\nThis project not only aims to simulate a PACS network on the physical layer with clinical workflow in mind, but to also simulate different PACS networks’ comprised of different network topologies and scales of implementation (small scale to larger scale networks). By further simulating these different topologies and scales of implementation, it is hoped that data obtained can be used to additionally compare and contrast different network topologies across different network sizes. By doing this, it is hoped to achieve optimized network architecture for the PACS context.\nThis project will focus solely on the physical layer within the PACS, more specifically the way the nodes within the network communicate. It is expected that caveats can be distinguished within topology design through packets being ‘dropped’. Packets are ‘dropped’ in communication due to bandwidth of the connection being reached and so the connection cannot support anymore traffic being transmitted.\nThe PACS network requires optimisation due to its increase in clinical use. More data is being transmitted across PACS network environments, and this may be patient records or patients’ medical images, without optimisation the network may suffer performance issues.\n2. Aims \u0026amp; Objectives # The goal of this project is to simulate different topologies in a PACS environment, retrieve meaningful data from the simulation, and then to use mathematical computation on the outputted data to compare and contrast the different topologies in order to discover an optimal network for PACS.\n2.1 Aims # Investigate common PACS network implantations to get an understanding of the different ways they are implemented.\nDesign and implement different topologies for the PACS network environment.\nRetrieve meaningful data from the simulations developed.\nStatistically analyse the data retrieved from the simulations through graphs and computation of the data.\nFrom the statistical analysis, find an optimal network for PACS.\n2.2 Objectives # Design and develop a simple PACS network simulation.\nDesign and develop simulations of different topologies.\nApply different throughput allowances for each topology.\nDevelop a way of retrieving meaningful data from simulations for analysis.\nDevelop a way to compute data outputted and display results in a meaningful way.\nCompare and contrast implementation of different topologies from the results obtained from the simulations.\nFind an optimised PACS network.\nSummarise the results of the project.\nPerform a critical evaluation of the project.\n3. Review of academic literature # The purpose of this project is to simulate and review different network topologies to define an optimised network for the PACS scenario. The scope of academic literature reviewed was in relation to the PACS network performance and measuring network performance using NS2. The literature reviewed is sorted by ascending publication year.\nUw PACS Prototype Performance Measurements, Computer Model, and Simulation\n(Panwar, et al., 1990) Discussed their early efforts on simulating different PACS network architectures by creating models with NS2. From their studies they identified parameters in their model which would prove useful for this project such as;\nImage Acquisition Time: This is the time taken to receive and display an image packet. The time differs depending on the source (MRI, CT scanner) as the image may be larger or consist of many images.\nLocal Disk Access Time: This is the read/write time for a packet on the local machine (workstation). Although the transfer rate was 0.25Mbyte/s, the rates have increased drastically in modern advances.\nCompression/ Decompression time: This is the time taken to compress or decompress image packets. This depends on the compression technique used, for example: JPG.\nTransmission Time: This is the time taken to transmit a packet between 2 nodes and is the result of a link between 2 nodes. This link could be a CAT5 cable or fibre optic cable.\nTheir research also discussed other parameters on their network such as traffic distributed from modularities on the network and the amount of images/packets in which these machines transmit. They were also able to discover bottlenecks with scaling up their own network (implementing a star topology), although the most considerable limitation discovered was the compression and decompression time of their hardware. Transmission time was found to not be a bottleneck in their research. The parameters mentioned can still be applied to more modern networks, however, as technology has been advanced over time (more optimised compression techniques and hardware), the bottlenecks they have found may not be a bottleneck in more recent PACS networks.\nPerformance Evaluation of a Picture Archiving and Communication Network Using Stochastic\n(Martinez, et al., 1990) Investigates star networks but also takes a range of workloads into consideration for measuring performance, more specifically implementing fibre-optic as the link between nodes, See Fig. 3. For example.\nComponents in their PACS network consisted of imaging equipment, viewing workstations, and a PACS archive system. The model simulated consisted of one star network, however, in reality a PACS network could consist of multiple star networks, for different departments.\nThe workflow used in simulations were a patient being admitted for a procedure session where images are generated from. The images taken during that session are stored locally on the imaging equipment. Once the procedures are complete, there is then a request for the images to be stored on the PACS archive system, which is then queued on that system. When the PACS archive system was ready to receive and store said images, it would then send a connection granted packet and transmission would begin.\nThe assumed workload described by (Martinez, et al., 1990) consisted of imaging equipment and workstations generating requests. There was two types of requests identified; the first which was requesting for the transfer of images and the second which requested for patient information. For their study, it was assumed that 94% of requests were for the transfer of images with the remaining 6% being for patient information. This was used to create probabilities of which type of request is sent at any time on the network. This adds another useful parameter to their simulation and helps represent a PACS with workload demands.\nIt was concluded that the passive star topology responded well with scaling (up to 35 machines), with minimal packet collisions. It was discussed that different methods of connecting multiple star networks is a field of research which could be expanded and studied. The connection of star networks could be implemented by a backbone network.\n*Picture Archiving and Communication System (PACS) Characteristic on Wired-line and Wireless Network for Traffic Simulation *\n(Chimmanee \u0026amp; Patpituck, 2013) Explores PACS traffic characteristics of wired and wireless networks. While wireless networks are out of the scope of this project, (Chimmanee \u0026amp; Patpituck, 2013) does introduce the parameter of packet loss in their simulations. Although it was found that wired networks did not experience packet-loss as prominently as wireless, it should be a parameter taken into consideration during the PACS Network Optimisation project as it is a factor experienced in a PACS network environment.\nPacket-loss was measured by distributing flows of data of packet sizes ranging from 54 bytes to 1518 bytes in flows of 1 to 6. The highest volume of packets were found to be in the range of 1024 to 1518 bytes, which accounted for 60% of data being transmitted. Second highest being packets within the range of 54 to 64 bytes with accounted for 30% of data being transmitted. The third highest was 512 to 1023 bytes which accounted for 9% of data being transmitted and packets between 128 and 511 bytes accounted for the final 1%.\nThis information will prove useful for assigning probabilities of the size of packets which will be transmitted through the PACS network, generating a more accurate simulation of traffic.\nConclusion\nWhile star networks have been extensively investigated by (Martinez, et al., 1990) and (Panwar, et al., 1990), there still is a gap in research of simulating multiple star networks with a backbone architecture. Possible parameters for the simulation have been identified, most prominently from (Panwar, et al., 1990)’s simulation of their own PACS network. It would appear that there is still more research that could be done in different topologies other than the most popular star network. As some of the literature was not published in more recent years, investigation into more recent protocols for connecting nodes in a LAN could be identified and explored. Overall, the literature shows there are gaps in relation to simulating modern PACS network topologies with increased scales.\n4. Methodology # This section will detail the methodology of the project including project management and the methodology behind software development and research methods. This will also include an analysis of the chosen toolsets and machine environments.\n4.1 Project Management # The PACS network optimisation project is based upon software development. Initial requirements are to be gathered which are unlikely to be changed due to the aims of the project being well defined early on. Due to the nature of the project, it does not involve stake-holders or a client and requirements are unlikely to be changed, allowing for a step-by-step approach to design, implementation and statistical analysis.\nThe project is to be approached in a sequential manner, and such, the Waterfall methodology would prove beneficial. The use of milestones alongside Waterfall would be advantageous to mark significant points along the project’s timeline. These points may include design of the PACS topologies, successful implementation of network simulation, meaningful data extracted from implementation or learning new Toolsets and Machine Environments. By implementation of milestones, the progress of project can be reviewed, ensuring success of the projects aims.\n4.1.1 Risk assessment chart # Risk Assessment # R1. Loss of data # Likelihood: Low Impact: High Mitigation: Perform regular backups on external storage after each work session. R2. Time taken away due to external commitments # Likelihood: Medium Impact: Medium Mitigation: Allocate time effectively and follow the Gantt chart to complete weekly tasks. R3. Ineffective implementation of simulations in NS2 # Likelihood: Low Impact: High Mitigation: Read NS2 documentation and research how to implement required features. R4. Difficulty using new toolsets or machine environments # Likelihood: Medium Impact: Medium Mitigation: Research the toolsets/environments to be used and allow time for learning. R5. Not achieving project milestones within the Gantt chart timescale # Likelihood: Medium Impact: High Mitigation: Review progress at each milestone to prevent delays and maintain project success. R6. Unsuitable data for statistical analysis # Likelihood: Low Impact: High Mitigation: Review progress regularly at each milestone to ensure alignment with initial project aims. 4.2 Software Development # The use of Waterfall is the most advantageous methodology for PACS network optimisation suitable to the characteristics of the project. This is due to the requirements for the project being very well defined and having a low chance of being changed during the duration of the project. Requirements are unlikely to change due to the project not having to cater for a client whom may change the aims of the project during its development life cycle which allows the project to advance through stages of development without the need to return to previous stages in the cycle. These characteristics are well suited to the waterfall methodology, as once a stage is completed, it is rarely returned to.\nThe PACS network optimisation project is to be carried out over a short time frame as this is where the Waterfall’s rigidity would prove beneficial and the methodology is best suited for shorter projects which will not be ongoing for longer timeframes. As Waterfall is sequential in regards to how its development cycle is completed, each phase is easily reviewable and important milestones can be established, allowing for effective time management which in return ensures a higher chance of project success.\n4.3 Research methods # The main aim of this project is to evaluate different PACS network topologies to establish an optimal network for PACS. To effectively evaluate the topologies simulated, a quantitative research approach was used to generate numerical data which can be transformed into useable statistical data. Quantitative research is appropriate for this project due to the requirement of quantifying behaviours of the network topologies in simulation. By using quantitative research, the project is able to use measurable data from the simulations to establish facts and discover patterns within research.\nA qualitative approach to research would not be appropriate for this project as qualitative is focused towards uncovering trends within thought and opinions to get a better understanding of the problem. Qualitative research would not benefit the project as the problem is already defined and it is more useful as a way to conduct primary exploratory research.\nDue to the nature of the project, it is essential that the data recorded is objective, in order to produce results from the simulations that can be used to gather results which can be further used to discover patterns.\nThe form in which data should be recorded is through intervals, because it is used to perform a meaningful analysis of the topologies and is important to test different values of throughput to compare the results.\nResults from the research conducted will be represented through graphs for each throughput of the simulations. Values of the throughput will increment from 100kb to 900kb in intervals of 100kb for each topology design, while also graphing the amount of packets dropped over time. By representing the data in this manner, it is hoped that patterns can be discovered and documented. The independent variable recorded will be time, whereas the dependant variables will be the amount of throughput and the number of packets dropped.\n4.4 Toolsets and Machine Environments # 4.4.1 Linux (Ubuntu 14.04 LTS) # The use of Ubuntu 14.04 LTS (Ubuntu, 2016) as the machines operating system to run the simulation toolset was decided based on the following requirements; Accessibility, Open-source, Programming environments, tool-sets available, compatibility of simulator.\nLinux (the architecture base of Ubuntu) is accessible to anyone whom can access the internet and is available in different flavours and versions. The choice of using Ubuntu was decided after reading academic literature in which the use was favoured between network simulation researchers.\nUbuntu is open-source, meaning the architecture of the operating system can be modified to the users’ requirements, allowing for a system which can be tailored to the demands of this project. The choice of the version Ubuntu 14.04 LTS was made because this version offers “Long-term Support”, meaning it comes with five years of security and maintenance updates. While this time period is a much longer time frame than this project requires, it allows for a wider availability of support with third-party applications (such as the simulator).\nUbuntu is installed with built-in programming environments and tool-sets which can interact with the simulation environment with more ease. This due to simulation environments on other operating systems that are not as out-of-the-box modifiable or open-source. While some features can be ported onto an operating system such as Windows, it is limited in comparison with some tool-sets not being available due to the operating system.\nNS2 was originally developed for Linux and later ported to Windows. Even though it is possible to use NS2 on the Windows operating system, it does lack toolsets and overall support for the program, which is due to the different programming frameworks for each operating system.\n4.4.2 Gawk # Gawk (GNU, 2011) is the GNU version of the UNIX Awk program (another stream editor). The basic function of Gawk is to search files for lines or other text-based files for patterns within the file. When a pattern is found pre-defined actions are taken and performed on that line of text.\nGawk was chosen to find dropped packets within the files outputted by the network simulator as the Gawk programming language is best suited for the task. Gawk was preferred over other programming languages because it is “data-event driven”. Data which is required to be outputted is defined and the action in which to operate on that data is also defined. Most other programming languages are “procedural”, meaning every step within that program has to be defined in great detail. It is usually quite difficult to define which data is to be processed, because of this, Gawk is preferred and implemented.\n4.4.3 Matlab # Matlab (MathWorks, 2016) is a high-performance programming language for technical computing which integrates computation, programming and visualisation of results, all within a Integrated development environment (IDE). Problems and solutions are expressed through mathematical notation.\nMatlab has the ability to compute large memory operations, unlike Excel, which cannot dynamically call array values. This would prove troublesome as there is a need to compute large arrays of data to visualise statistical results.\nWhile statistical programs such as Excel are useful, the use of Excel would not be suitable for analysing large data sets produced by network simulation. Other limitations of Excel compared to Matlab include slow computation and limited graphing capabilities.\nSome of Matlab’s advantages include the availability of sophisticated analysis packages, functions, and the ability to create and execute scripts from a command line interface (CML) within the IDE. The ability to create and execute scripts is essential to process large quantities of data.\n4.4.4 NS2 (Network Simulator 2) # NS2 (nsnam, 2014) is the second version of the NS open source network simulators and is respectively one of the most popular ones, as previously discussed. NS2 is widely used in academia and has gotten a lot of packages available for use and because of its popularity, documentation is updated often and features are well documented.\nNS2 is an object-oriented, discrete event driven simulator, using C++ and Tcl script language with Object-oriented extensions (OTcl). NS2 was originally developed at the University of California-Berkely. The main reasoning behind implementing C++ and OTcl is because of C++’s efficiency when executing a design but not when it comes to visualising and graphically displaying the designed network. As C++ is efficient, the code that is written and compiled does reduce packet and event processing time. However, as C++ is not very effective at visualisation, OTcl is used for controlling simulation scenarios, set up network topology, and to schedule events while C++ is used to define protocols. This combination of the 2 languages proves very effective.\n5. Design, Development and Evaluation # This section of the project aims to express an explanation of the design, development and evaluation of the artefact created with results generated from the simulations.\n5.1 Requirements collection and analysis # Requirements for the project were identified early within the projects timeframe. The requirements were a result of background reading around existing implementations of PACS and these requirements were analysed in regards to how they could benefit this project.\n5.2 Implementation/design # This section of the document explains the implementation and design behind each of the toolsets and machine environments executed in the project.\n5.2.1 Linux Implementation # Before the project could simulate any network topologies, it was essential to install a base operating system to which toolsets could be executed upon. The version of Linux chosen for the base operating system was Ubuntu 14.04 for the reasons discussed in section 4.4.1 Linux (Ubuntu 14.04 LTS). Once the operating system was installed on the host machine, NS2 along with Gawk were installed onto the operating system so that the project could begin.\n5.2.2 NS2 Implementation # Before work could begin on the PACS network, it was important to address R3 from the risk assessment chart (refer to section 4.1.1). This risk was an ineffective use of NS2 and was unable to understand how to use the toolset, so a prototype of 2 nodes communicating was programmed in Tcl then executed by NS2 as shown in Figure 4.\nThe steps in which the creation of a Tcl script to simulate a network are:\nTopology definition: definition of the relationships between nodes.\nModel development: models are added to simulation.\nNode and link configuration: models set their default values (for example, the size of packets sent by an application or bandwidth of a link between nodes).\nExecution: network is simulated and events are run, data requested is logged.\nPerformance analysis: once the simulation is completed and data is available as a time-stamped event trace.\nGraphical Visualisation: Visualisation of the simulation is achieved through the event trace file.\nObserving Packets being dropped\nOnce an understanding of how to write and then execute a basic Tcl script through NS2 was established, experimentation on how packets where dropped took place. Packets are dropped due to the throughput of data exceeding the bandwidth of the connection between nodes. An example of packets being dropped in a simulation is shown in Figure 5.\nAt this point it was important to understand the structure of the trace file create by NS2 when the simulation was being executed, so that further into the project, dropped packets could be identified. Figure 6 highlights what a packet being dropped appears like within the tracefile.\nAfter reading the NS2 documentation on trace files, it was concluded that the format of the data in the trace files being created are as shown in Table 1.\nEvent type time from to type Packet size flow_id src dst Sequence number of packet Packet id Table 1: The format of data in the trace file\nUnderstanding the trace file is essential for later on in the project where meaningful information needs to be extracted from the trace files to create statistical data to be analysed.\nAddition of nodes to simulation\nThe addition and connection of nodes is what defines the topologies. At this point in the project, there is an understanding of Tcl files, the trace files being created and also the NS2 simulation environment. It is at this moment when topologies start to take shape. **Error! Reference source not found.**Figure 1 shows a simple Tcl script in which 3 nodes are connected with the code which connects them.\nCreation of procedures within the Tcl script\nNow that there is an understanding of how to create topologies within Tcl, it is important at this stage to create the procedures which will be used across the three topologies. The procedures defined include: Traffic generation, traffic sources, bandwidth recording, and the timing in which the sources will generate packets. Each procedure will be discussed in their respective sections below.\nTraffic generation\nTraffic generation defines a procedure that attaches a UDP agent to a previously defined node and attaches the Expoo-traffic-generator to the agent with the characteristic arguments; \u0026lsquo;size\u0026rsquo; for packet size, \u0026lsquo;burst\u0026rsquo; for burst time, \u0026lsquo;idle\u0026rsquo; for idle time and \u0026lsquo;rate\u0026rsquo; for burst peak rate. The procedure connects the source with the previously defined traffic sink and returns the source object.\nThe burst peak rate is the argument used to define how much data can be transmitted from the source node to the destination node, this argument is used when testing each of the topologies ability to transmit and receive different volumes of packets and will be incremented from 100k to 900k, in intervals of 100k. Figure 8 shows the source code for this procedure.\nFour traffic generation procedures are defined so that each traffic source has a unique flow id. the flow id is used to identify the different flows of packets.\nBandwidth recording\nThe bandwidth recording procedure periodically records the bandwidth received by the four traffic sinks sink0/1/2/3 and writes it to the four tracer files out0.tr/out1.tr/out.tr2/out3.tr. These files are to be later used when computation of the throughput is achieved. Figure 9 shows the source code for bandwidth recording.\nTraffic sources\nIn this procedure the sources of traffic are assigned to nodes with sink’s so that bandwidth data can be record to trace file and also the arguments: \u0026lsquo;size\u0026rsquo; for packet size, \u0026lsquo;burst\u0026rsquo; for burst time, \u0026lsquo;idle\u0026rsquo; for idle time and \u0026lsquo;rate\u0026rsquo; for burst peak rate are defined. This is where the burst peak rate is defined for each of the traffic sources. Figure 10 shows the source code for the traffic sources procedure.\nEvent timing\nThis section of the Tcl script is where the events are controlled. The recording procedure is started at the beginning of the simulation and traffic sources are started incrementaly with a delay of 5 seconds between each source, generating data as shown in Figure 11. The traffic sources are started with incremenatlly allowing the throughput on the simulation to be slowly increased. This is so the different topologies can be analysed with varying amounts of traffic during the simulation.\nCreation of the different topologies\nNow that the procedures are defined for the simulation, the different topologies are created. The procedures are not changed other than the incrementing throughput values for the traffic sources, this is to ensure data outputted by the simulations can be compared fairly and remain valid.\nStar topology\nThe star topology consists of a central node, to which other nodes are connected to. The central node provides a connection point to which all the other nodes can communicate through. The central node represents a hub or a switch, while the nodes connected to the central node represent the modularity’s on a PACS network such as workstations requesting image files or MRI scanners sending data to be stored on the PACS. Figure 12 shows the star topology defined in NS2.\nRing topology\nThe ring topology consists of nodes connected to exactly two other nodes, forming a continuous pathway for packets to travel around the network. Packets travel around the network passing being handled by each node until they reach their destination node, whereas in the star topology, a central node is responsible for handling the packets being transmitted. Figure 13shows the ring network as visualised by NS2.\nMesh topology\nThe mesh topology consists of nodes which all cooperate in the distribution of data within the network. In this instance all nodes are connected and the packets are relayed to their destination node by the most direct route. Figure 14 visualises the mesh topology in NS2.\n5.2.3 Gawk Implementation # Gawk was implemented to navigate through the trace file of the simulation to record times when packets were dropped along with a total count of when packets were dropped. This was implemented so that meaningful data could be extracted from the simulations executed. The Gawk script takes the data extracted from the trace file and then inserts it into a output file in “.txt” format, so that it can then be graphed within Matlab to provide statistical analysis of the packets dropped in each simulation. The gawk script is executed through Linux’s CLI where the arguments “Gawk –f ‘Gawkscript.awk’ ‘tracefile.tr’” are parsed.\nThe Gawk script first allocates each tab of data within the trace file to variable in the script, which then is used in an ‘if’ statement that navigates through each line of the trace file searching for the event which indicates the dropping of a packet. After discovering the packet drop event, the time of when the packet is dropped is recorded and a counter increments by 1 to record the number of packets dropped. Figure 15 shows the Gawk script implemented.\n5.2.4 Matlab Implementation # Matlab was implemented as a way to compute data into meaningful statistical data which can be analysed to find patterns within the simulations. These patterns are used to find an optimised network topology for the PACS network environment.\nMatlab was used to generate graphs which plot throughput data as well as plotting packet loss data.\nPlotting Throughput data\nThe Matlab script used to plot throughput data loads in the out1.tr/out2.tr/out3.tr/out4.tr files which are generated by the Bandwidth recording function to then plot the data onto a graph. Matlab achieves this by assigning the files to arrays which are then plotted using the built-in function ‘plot’. When the script is executed, the program displays the generated graph, which can then be saved. Figure 16 shows the source code for the throughput data script.\nPlotting Packet loss data\nThe Matlab script was used to plot the packet loss over time graph. Firstly, this reads the output file from the Gawk script into an array and then plots the times that packets are dropped to a line chart. This can then be used to compare and contrast the different topologies with the incremented throughput values. Figure 17 shows the source code for the throughput data script.\n5.3 Evaluation # From the aims defined in section 2.1, three PACS network topologies were designed and simulated using NS2 to generate meaningful data to assist with discovering an optimised PACS network. The data generated from the simulations was successfully gathered in a manner which can allow for statistical computation. In order to determine an optimised PACS network, the statistical data generated needs to be evaluated. This section will evaluate the results for each of the three topologies to arrive at a conclusion of the project.\n5.3.1 Star network topology # The star network topology was successfully simulated in NS2 with packets from the modularity’s (PC’s and PACS) being routed through the switch, from their source node to their destination node.\nThe star network topology had shown throughput spiking above the bandwidth allowance (indicating packets being dropped), from the throughput value of 400kb through to the value of 900kb (see appendices 8.1). The caveat within the star topology from the simulations was that the central node (switch) was not able to route the packets to their destination as there was a higher throughput that was being passed through the switch. This resulted in packets being lost more throughout the incrementing burst peak rates as seen in the appendices 8.2.\n5.3.2 Ring network topology # The ring network topology was successfully simulated in NS2 with packets from the nodes being routed around the ring network. The packets were routed through each node as they travelled from their source node to their destination node.\nThe ring network topology was showing throughput spiking above the bandwidth allowance (an indication of packets being dropped), from the throughput value of 600kb through to the value of 900kb (see appendices 8.3). The caveat of the ring topology was that in order for the packets to be routed from the source node to the destination node, they would have to traverse through over nodes, which caused packet loss (see appendices 8.4). Packet loss was due to bandwidth of links between the nodes being used to route multiple traffic sources, which is an unavoidable problem due to the nature of the ring topology.\n5.3.3 Mesh network topology # The mesh network topology was simulated within NS2 with packets from the nodes being routed around the mesh network using the fastest route. All nodes in the mesh topology simulation were connected, which allowed for packets to be routed around the network with zero packets being dropped. The bandwidth of the links between nodes shows almost no spiking of the throughput. The caveat with implementing the mesh network topology would be that all nodes would need to be connected, which would be unrealistic in a PACS network environment and impossible to scale past a few machines.\nThe reasoning behind why it would be unfeasible to use the mesh topology on more than a few machines would be that the requirement of many network interface cards (NIC’s), as well as the shear amount of cabling, are needed to link the nodes.\n6. Project Conclusion # The aim of this project was to find an optimal network for the PACS network environment through statistical data obtained from simulations of different network topologies.\nThe first aim was to investigate common PACS network implementations. From background research into the history of PACS (Huang, 2011) and (Lemke, 2011), the project was able to define requirements on how different modularity’s on the network communicate with one another and their respective workflows.\nThe second aim of the project was to design and implement different topologies for the PACS network environment. The design of the different topologies were a result of researching different network topologies and then applying the topologies to a PACS environment. The topologies were applied to the PACS environment through knowledge gained from researching into the history of PACS implementations.\nThe third aim of the project was to gather meaningful data from the simulations developed. This was achieved through researching appropriate toolsets and machine environments to then developing solutions to gathering meaningful data from the trace files generated from the topology simulations.\nThe forth aim was to statistically analyse the data retrieved from the simulations through graphs and computation of the data. This aim was achieved through meaningful data being extracted from the data generated from the simulations being executed. This was then statistically analysed through the toolsets which were earlier identified in section 4.4.\nFinally, the fifth aim of the project was to find an optimal network for PACS. From the evaluation of the network topologies simulation in section 5.3, the optimal network for reducing packet loss through the testing of incremental burst peak throughput rates, would be the Mesh topology, however, this may only be applicable through simulation. As discussed in section 5.5.3 this topology would require a considerable amount of network interface cards on each modularity within the network, to a point where this may not be feasible to implement in a PACS environment and certainly unfeasible to scale up the network size as many links would have to be made within nodes on the network.\n6.2. Further work # As the project was conducted on a strict timescale some features were unable to be implemented. Further research could involve gathering more recent data of PACS network specifications, such as the size of packets within a PACS network, and the bandwidth of links between methodologies.\nOther possibilities include:\nApplication of network protocols found especially within PACS to simulate a more realistic PACS network.\nTesting of varied packet sizes to produce a median and standard deviation of throughput during statistical analysis.\nAs discussed in conclusion of section 6, a hybrid network, combining characteristics of the popular star topology with the resilience of the mesh topology could be further investigated and researched as a potential solution.\n6.3 Reflective Analysis # Reflecting on the project as a whole I would consider it a success. The aims and objectives defined at the start of the project were completed, bearing in mind they could have been furthered. The scope of the project was sensible but also challenging, however, I have gained a lot of knowledge through research conducted on PACS.\nThroughout the project, it was troublesome to find documentation on recent PACS network information such as bandwidth of the network and the clinical workflow of a PACS network. This information would have been useful to create a more accurate simulation of a PACS network.\nHaving little knowledge of the toolsets and machine environments implemented during this project made it very difficult to progress at first but certainly made the project rewarding as I progressed through each milestone.\nDue to having no prior knowledge of NS2 when starting to simulate networks, I ran into many problems and it proved very tricky to begin with. NS2 does not have an IDE, meaning the only feedback on errors was from the CML on Linux. This made problem solving tedious, however, as NS2 is such a well-established network simulator, there was a lot of documentation regarding its implementation, and this proved helpful in self-learning on how to use the software.\nI feel as though more quantitative data could have been collected through implementation of generating random packet sizes for each simulation, however, the reason behind not being able to implement such a feature was as a result of having to manually run each simulation, which proved to take a lot of time, especially considering the simulations had to be re-run every time the burst peak rates were changed in the traffic generator procedure.\nDespite all the problems faced throughout this project, I feel as the project was an excellent learning experience for myself. The ability to see the progress I have made over the projects time frame gives me satisfaction of the work I have accomplished and I feel the project was successful.\nReferences # Chimmanee, S. \u0026amp; Patpituck, P., 2013. *Picture archiving and communication system (PACS) characteristic on wired-line and wireless network for traffic simulation. *s.l., IEEE, pp. 589-594.\nGNU, 2011. [Online] Available at: https://www.gnu.org/software/gawk/ [Accessed 14 April 2016].\nHuang, H., 2011. Short history of PACS. Part I: USA. *European Journal of Radiology, *78(2), pp. 163-176.\nLemke, H. U., 2011. Short history of PACS (Part II: Europe). *European Journal of Radiology, *78(2), pp. 177-183.\nMartinez, R. et al., 1990. *Performance evaluation of a picture archiving and communication system using stochastic activity networks. *s.l., International Society for Optics and Photonics, pp. 167-178.\nMathWorks, 2016. [Online] Available at: http://uk.mathworks.com/?refresh=true [Accessed 14 April 2016].\nnsnam, 2014. *The Network Simulator - ns-2. *[Online] Available at: http://www.isi.edu/nsnam/ns/ [Accessed 14 April 2016].\nPan, J., 2008 . *A Survey of Network Simulation Tools: Current Status and Future Developments. *[Online] Available at: http://www.cse.wustl.edu/~jain/cse567-08/ftp/simtools/index.html#51 [Accessed 12 November 2015].\nPanwar, R. et al., 1990. *UW PACS prototype performance measurements, computer model, and simulation. *s.l., International Society for Optics and Photonics, pp. 869-880.\nSantos, M. et al., 2015. DICOM Metadata Access, Consolidation and Usage in Radiology Department Performance Analysis. A Non-proprietary Approach. *Procedia Computer Science, *pp. 651-658.\nUbuntu, 2016. [Online] Available at: https://wiki.ubuntu.com/TrustyTahr/ReleaseNotes?_ga=1.70783760.1632602236.1460635579 [Accessed 14 April 2016].\n","date":"10 July 2016","externalUrl":null,"permalink":"/posts/pacs-network-optimisation-dissertation-project/","section":"Blog Posts","summary":"A project that simulates PACS network topologies using NS2 to evaluate performance under increasing traffic loads. Packet‑loss data is extracted, analysed, and visualised to compare designs. Results show the mesh topology performs best under heavy throughput, though combining topologies warrants further study.","title":"PACS Network Optimisation - Dissertation Project","type":"posts"},{"content":" Introduction # A Picture Archive Communication System (PACS) is a system which is comprised of medical imaging scanners (for example: CT or MRI scanner), storage mechanisms, information communication technologies, displays and clinical workflow. A PACS network is the combination of PACS systems and nodes (computers) accessing storage of images, at any time image files (often many) can be requested for viewing or being saved to storage mechanisms.\nThe concept of digital communication and digital radiology was introduced in the late 1970s and early 1980s, the beginning of PACS was attributed to development in the USA where research was contributed by laboratories within universities and small companies in the private industry which had entered the field in 1982. In the early years of PACS evolution research and development varied on region in which it was conducted; PACS research in North America was largely supported by government agencies and manufacturers (Huang, 2011). Research in Europe was supported through multi-national efforts, PACS components weren’t as accessible in Europe as N.A. and Asia so research emphasized on modelling and simulation. PACS research in Asia was led by Japan which treated the research as a national project, resources were distributed to many manufacturers and university hospitals (Lemke, 2011).\nA PACS network often has many workstations and modularity’s sending large amounts of packets of information around, being requested for viewing or being stored in PACS archives, (Santos, et al., 2015) highlights this, although their research was oriented around DICOM metadata, there was a clear pattern of more images being produced year by year, all of which needs to be stored with redundancy in mind. With all this data being transmitted and received on the network, optimisation of how these nodes communicate would prove valuable for increased network performance.\nDue to large image files being requested or being stored, network bandwidth can often be limited which causes slowness of the network, slowing workflow and potentially causing a lower quality of service for patients. As a PACS network can often consist of multiple storage mechanism’s which allow for data redundancy, bandwidth can be further limited.\nThis project aims to simulate a PACS network with clinical workflow in mind, but to also simulate a PACS network comprised of different network topologies and scales of implementation (small scale to larger scale networks). Further simulating these different topologies and scales of implementation it is hoped that useful data which can be obtained to further compare and contrast different network topologies across different network sizes, by doing so it is hoped to achieve an optimized network architecture for the PACS context.\nNetwork Simulators # Research into finding a suitable application to simulate these networks had some factors to be taken into consideration, the application has to be open source so that modifications can be made if and when necessary so that the simulation is tailored to PACS and clinical workflow, because of this proprietary software would not be suitable as it is often not possible to see source code and may have restrictions on its use.\nThe simulation software needs to be capable of modelling different network topologies, different network sizes, logging the packets (data which is sent between nodes) being transmitted on the network, visualisation of packet flow would also be highly beneficial for analysis and to also output the performance of the network in metric values which can be analysed and later used for comparison.\nThere are numerous research applications which cater to the purpose of simulating networks and network traffic analysis, through researching for a suitable application the most common were: NS (Often referred to as NSNAM), OPNET and NetSim. Each come with their similarities and differences but ultimately their aims are the same.\nNS\nNS (Network Simulator) is the name for a series of applications which are discrete event network simulators (Events take place at a particular time and affect the state of the system), NS is primarily used for research and there is a lot of documentation which can be used assist the development of this projects aims. NS is licensed as GPLv2 which means it is publically available for research, development, and use, so any modifications to its source code can be made if necessary.\nOPNET\nOPNET (Optimized Engineering Tools) is a network simulation tools with many features and tools which include; Packet definition (used to define protocols), node modelling (used to specify network component interfaces, for example; Workstations, switches, servers…), a project window (used to define network topologies, node links etc..) and a simulation window which has the capability to capture and show results of an simulation. OPNET is a commonly used application in network research and development, however, as OPNET is proprietary software accessing and modification of the programs source code is not an option and to use this software a license would be required, making OPNET not suitable for this project.\nNetSim\nNetSim is another popular network simulator which is used for network research and development. NetSim offers network modelling and simulation services in many protocols and technologies, the protocol libraries which offer this are available with C source code so they can be modified to suit requirements at hand. NetSim is packaged with a built-in development environment which is used to bridge user’s code with the protocol libraries and simulation kernel, This environment also allows for a simulation to be started, and single-step, step-in and step-over functions to be run by using pre-determined breakpoints marked by the user. Whilst NetSim does offer C source code for its protocol libraries it is proprietary software which will require a license to be used, making NetSim not suitable for this project.\nComparison of simulators\nWhilst the discussed network simulators have functionality which is similar across all of them, they do have distinct differences, such as how user created network scenarios are parsed through their respective simulation kernels, NS requires a lot of knowledge to create and simulate a network as it is mainly command line oriented and files need to be wrote outside of a dedicated development environment, however, as NS is quite prominent in research papers I found a lot of documentation and tutorials on how to use the software. OPNET uses an GUI which does reduce the time that needs to be taken to learn the software, as OPNET is considered a mature tool for network simulation there is a lot of documentation and tutorials available online, however, for this project it would be beneficial to be able to access and modify source code, which cannot be done with OPNET, making it unfavourable for this project. NetSim is another popular solution to simulate networks and provides a useful GUI and built-in development environment for creating scenarios, NetSim does offer open source libraries for its protocols but can be restricting as it is propriety software which will require licencing for this project, making it unfavourable compared to NS which is completely open source and does not require licensing to use.\nNS2 (Network Simulator 2) # NS2 is the second version of the NS open source network simulators and is respectively one of the most popular, as previously discussed. NS2 is widely used in academia and has got a lot of packages available for use, because of NS2’s popularity documentation is updated often and features are well documented.\nNS2 is an object-oriented, discrete event driven simulator, using C++ and OTcl (Tcl script language with Object-oriented extensions). NS2 was originally developed at the University of California-Berkely. The main reasoning to implementing C++ and OTcl is because of C++’s efficiency when implementing a design but not when it comes to visualising and graphically displaying the designed network, as C++ is efficient the code written and compiled does reduce packet and event processing time. Because C++ is not very effective at visualisation OTcl is used for controlling simulation scenarios, set up network topology and to schedule events while C++ is used to define protocols, this combination of the 2 languages proves very effective.\nReview of academic literature # The purpose of this project is to simulate and review different network topologies to define an optimised network for the PACS scenario, the scope of academic literature reviewed was in relation to PACS network performance and measuring network performance using NS2. The literature reviewed is sorted by ascending publication year.\nUw PACS Prototype Performance Measurements, Computer Model, and Simulation\n(Panwar, et al., 1990) Discussed their early efforts on simulating different PACS network architectures by creating models with NS2. From their studies they identified parameters in their model which would prove useful for this project such as;\nImage Acquisition Time: This is the time taken to receive and display an image packet, this time differs depending on the source (MRI, CT scanner) as the image may be larger or consist of many images.\nLocal Disk Access Time: This is the read/write time for a packet on the local machine (workstation), while their transfer rate was 0.25Mbyte/s in modern advances transfer rates have increased drastically.\nCompression/ Decompression time: This is the time taken to compress, decompress image packets, this depends on the compression technique used, for example: JPG.\nTransmission Time: This is the time taken to transmit a packet between 2 nodes. Transmission time is the result of a link between 2 nodes this link could be a CAT5 cable or fibre optic cable.\nTheir research also discussed other parameters on their network such as traffic distributed from modularity’s on the network and the amount of images/packets in which these machines transmit. Whilst they were able to discover bottlenecks with scaling up their own network (implementing a star topology). The most considerable limitation discovered was the compression and decompression time of their hardware, transmission time was not found to be a bottleneck in their research. The parameters mentioned can still be applied to more modern networks, however, as technology has been advanced over time (more optimised compression techniques and hardware), the bottlenecks they have found may not be a bottleneck in more recent PACS networks.\nPerformance Evaluation of a Picture Archiving and Communication Network Using Stochastic\n(Martinez, et al., 1990) Investigates star networks but also takes a range of workloads into consideration for measuring performance, more specifically implementing fibre-optic as the link between nodes, See Fig. 3. For example.\nComponents in their PACS network consisted of imaging equipment, viewing workstations and a PACS archive system, the model simulated consisted of one star network, in reality a PACS network could consist of multiple star networks, for different departments.\nThe workflow used in simulations was a patient is admitted for a procedure session, images are generated from that session. The images taken during that session are stored locally on the imaging equipment, once the procedures are complete there is then a request for the images to be stored on the PACS archive system, which is then queued on that system. Once the PACS archive system was ready to receive and store said images, it would then send a connection granted packet and transmission would begin.\nThe assumed workload described by (Martinez, et al., 1990), consisted of imaging equipment and workstations generating requests. There was 2 types of requests identified; the first requesting for the transfer of images and the second which requested patient information. For their study it was assumed that 94% of requests are for the transfer of images with the remaining 6% being for patient information. This was used to create probabilities of which type of request is sent at any time on the network, which adds another useful parameter to their simulation and helps to represent a PACS with workload demands.\nIt was concluded that the passive star topology responded well with scaling (up to 35 machines), with minimal packet collisions. It was discussed that different methods of connecting multiple star networks is a field of research which could be expanded and studied. The connection of star networks could be implemented by a backbone network.\n*Picture Archiving and Communication System (PACS) Characteristic on Wired-line and Wireless Network for Traffic Simulation *\n(Chimmanee \u0026amp; Patpituck, 2013) Explores PACS traffic characteristics of wired and wireless networks, while wireless networks are out of the scope of this project, (Chimmanee \u0026amp; Patpituck, 2013) does introduce the parameter of packet loss in their simulations. While it was found that wired networks did not experience packet-loss as prominently as wireless, it should be a parameter taken into consideration during the PACS Network Optimisation project as it is a factor experienced in a PACS network environment.\nPacket-loss was measured by distributing flows of data, packet sizes ranging from 54 bytes to 1518 bytes in flows of 1 to 6, the highest volume of packets where found to be in the range of 1024 to 1518 bytes, which accounted for 60% of data being transmitted. Second highest being packets within the range of 54 to 64 bytes with accounted for 30% of data being transmitted. Third highest being 512 to 1023 bytes which accounted for 9% of data being transmitted and packets between 128 and 511 bytes accounted for the final 1%.\nThis information will prove useful for assigning probabilities of which size packet will be transmitted through the PACS network, generating a more accurate simulation of traffic.\nConclusion\nWhile star networks have been extensively investigated by (Martinez, et al., 1990) and (Panwar, et al., 1990), there still is a gap in research of simulating multiple star networks with a backbone architecture. Possible parameters for the simulation have been identified, most prominently from (Panwar, et al., 1990)’s simulation of their own PACS network. It would appear that there still could be more research into different topologies other than the most popular star network. As some of the literature was not published in more recent years, investigation into more recent protocols for connecting nodes in a LAN could be identified and explored. Overall the literature shows there are gaps in relation to simulating modern PACS network topologies with increased scales.\nAims of the project # Below the project has been broken down into small milestones which progress of each milestone can be monitored easier and achieved, further to this there is a breakdown and brief description of each milestone.\nInvestigate what a PACS is It is planned to investigate what a PACS is and how it functions in the real-world application.\nResearch common PACS network implementations Research how PACS networks are currently implemented to get an understanding of the different ways they are implemented.\nResearch NS2 and read through documentation NS2 has a steep learning curve so reading documentation on NS2 will be vital to this project being successful, as NS2 is well established there are a lot of up-to date tutorials and support.\nSimulate a simple PACS network To start off it is planned to simulate the up most basic PACS network as previously seen in Fig. 1. This will be the starting point to other more complex simulations, involving different topologies and scales.\nProcess of creating a simulation: The process of creating a simulation can be broken down into 7 steps which can be seen bellow, each of these steps will need to be followed for a successful implementation of a simulation\nTopology definition: define relationships between nodes.\nModel development: models are added to simulation.\nNode and link configuration: models set their default values (for example, the size of packets sent by an application or bandwidth of a link between nodes).\nExecution: network is simulated and events are run, data requested is logged.\nPerformance analysis: once the simulation is completed and data is available as a time-stamped event trace.\nGraphical Visualization: visualisation of the network is achieved.\nImplement simulations of other network topologies, such as star, mesh, server-client and ring etc.)\nFollowing the process of creating a simulation as discussed above, networks of different topologies will be simulated.\nGradually increase scale of networks From implementation of different network topologies, the scale of said simulations will be gradually increased, to see how each topologies handles traffic.\nCompare and contrast implementation of different topologies The performance of the different network topologies will be assessed from results obtained from the simulations.\nCompare and contrast different topologies in conjunction with different scales Measure the performance of the different network topologies by also with the factor of an increased scale.\nFind an optimised PACS network Once all the milestones above are completed it is hoped that an optimised PACS network can be identified and documented.\nProject plan # A project plan is essential for a successful project, risks need to be identified as well as risk mitigation, below is the Gantt chart in which time has been allocated for each of the aims of the project, as well as, a risk matrix which identifies possible risks to the project and how they may be mitigated.\nIt is planned to schedule weekly meetings with the project supervisor to ensure the goals of the project are met on time and any issues are mitigated.\nGantt chart # The Gantt chart below is comprised of summerised tasks with a timeframe in which they should be completed, Some tasks overlap as they may be finished early in the week and the next task will be started.\nProject risks # Project Risk Assessment # Risk Register # Risk Assessment # 1. Loss of data # Likelihood: Low Impact: High Mitigation: Perform regular backups on external storage after each work session. 2. Time taken away due to other assignments and exams # Likelihood: Medium Impact: Medium Mitigation: Allocate time effectively and follow the Gantt chart to complete weekly tasks. 3. Ineffective implementation of simulations in NS2 # Likelihood: Low Impact: High Mitigation: Read NS2 documentation and follow tutorials. 4. Difficulty using a new operating system # Likelihood: Medium Impact: Medium Mitigation: Use online tutorials and support forums to troubleshoot issues. 5. Not achieving initial goals # Likelihood: Medium Impact: High Mitigation: Hold regular meetings with the project supervisor and follow the Gantt chart. 6. Slower-than-expected progress with NS2 # Likelihood: Low Impact: High Mitigation: Use a different network simulator based on research into alternatives. References # Chimmanee, S. \u0026amp; Patpituck, P., 2013. *Picture archiving and communication system (PACS) characteristic on wired-line and wireless network for traffic simulation. *s.l., IEEE, pp. 589-594.\nHuang, H., 2011. Short history of PACS. Part I: USA. *European Journal of Radiology, *78(2), pp. 163-176.\nLemke, H. U., 2011. Short history of PACS (Part II: Europe). *European Journal of Radiology, *78(2), pp. 177-183.\nMartinez, R. et al., 1990. *Performance evaluation of a picture archiving and communication system using stochastic activity networks. *s.l., International Society for Optics and Photonics, pp. 167-178.\nPan, J., 2008 . *A Survey of Network Simulation Tools: Current Status and Future Developments. *[Online] Available at: http://www.cse.wustl.edu/~jain/cse567-08/ftp/simtools/index.html#51 [Accessed 12 November 2015].\nPanwar, R. et al., 1990. *UW PACS prototype performance measurements, computer model, and simulation. *s.l., International Society for Optics and Photonics, pp. 869-880.\nSantos, M. et al., 2015. DICOM Metadata Access, Consolidation and Usage in Radiology Department Performance Analysis. A Non-proprietary Approach. *Procedia Computer Science, *pp. 651-658.\n","date":"10 July 2016","externalUrl":null,"permalink":"/posts/project-proposal-pacs-network-optimisation/","section":"Blog Posts","summary":"A brief introduction to PACS, its history, and the growing need to optimise PACS network performance as imaging data increases. The project reviews network simulators, selects NS2 for its open‑source flexibility, surveys relevant literature, and outlines a plan to model and compare different network topologies to identify an optimised PACS network design.","title":"PACS Network Optimisation - Project Proposal","type":"posts"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/picture-archive-communication-system/","section":"Tags","summary":"","title":"Picture Archive Communication System","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/proccessing/","section":"Tags","summary":"","title":"Proccessing","type":"tags"},{"content":"","date":"10 July 2016","externalUrl":null,"permalink":"/tags/project/","section":"Tags","summary":"","title":"Project","type":"tags"},{"content":"Welcome to my corner of the digital world! I\u0026rsquo;m Joshua Robbins, and I\u0026rsquo;m thrilled to give you a glimpse into my professional journey, passions, and interests.\nProfessional Background # I currently work as an Senior IT Security Analyst, where my role revolves around safeguarding systems, users and customers.\nMy responsibilities include:\nIncident Response, handling and mitigating security incidents. Event Monitoring and Analysis. Vulnerability Management. Before my venture into IT security, I wore the hat of an Infrastructure Engineer. During this chapter of my career, I undertook and completed diverse infrastructure projects, each a puzzle I enjoyed solving. As a 2nd/3rd line incident infrastructure support resource, I learned the art of rapid problem-solving. I was the bridge between challenges and solutions, presenting problems to the infrastructure team and guiding them towards effective resolutions. My role also included the administration of systems, mainly Windows environments, and I ensured compliance with established standards such as ISO 27001 by conducting internal audits and maintaining policies.\nEducation and Expertise # My educational foundation includes a BSc in Computer Science, a journey that instilled in me the core principles of this ever-evolving field. In addition to my formal education, I hold industry-recognized certifications, including CompTIA Security+ and Azure Administrator AZ-104.\nPassions and Hobbies # Beyond my professional life, I\u0026rsquo;m a passionate enthusiast of technology and security. My home lab is my playground where I experiment with new concepts, tinker with different setups, and continually expand my knowledge. I find joy in dissecting emerging security trends and staying ahead of potential threats – it\u0026rsquo;s a dynamic field that keeps me on my toes.\nWhile the digital realm is my primary domain, I also have a deep appreciation for the natural world. Hiking across the picturesque landscapes of the UK is my way of unwinding.\n","date":"5 June 2016","externalUrl":null,"permalink":"/pages/about-me/","section":"Pages","summary":"","title":"About Me","type":"page"},{"content":"The Hallgate Timber website remodel is a project I am undertaking whilst working at the company, it is an in-house project which is allowing for constant feedback from the client. This project will be the first web-development project undertaken since my days at college (3 years ago), so this really should be a nice refresher/ learning experience.\nThe problem with the current website is that the website doesn\u0026rsquo;t really follow any design schema. content is added without a lot of thought into design. whilst the website is functional and does serve its purpose to a point, there is a lot in which can be improved upon. Navigation on the website needs improvement, after creating a sitemap to see what is being hosted, put simply its confusing.\nThe first version of the new Hallgate Timber website adds a little more CSS style than before, but while keeping the functionality. One of my aims with the new site is to keep the sites depth to a minimum, so all of the websites content is easily found, something not so easily done on the previous website.\nHTML Tables # HTML tables are a common implementation on the old website, whilst they do serve their purpose, DIV tag tables are the preferred way to go nowadays.\nBefore:\nAfter:\nWhilst there isn\u0026rsquo;t a dramatic different in the appearance of over a div table, there are advantages to useing div tags, such as for users\u0026rsquo; with screen readers and other accessible browsers.\n","date":"5 June 2016","externalUrl":null,"permalink":"/posts/hallgate-timber-website-remodel/","section":"Blog Posts","summary":"A website remodel project for Hallgate Timber focused on improving structure, navigation, and accessibility. The original site lacked a coherent design and relied heavily on outdated HTML tables, making content difficult to find. The redesign introduces cleaner CSS styling, a shallower site hierarchy, and modern DIV‑based layouts to improve usability, accessibility, and maintainability while preserving the site’s core functionality.","title":"Hallgate Timber Website Remodel","type":"posts"},{"content":"","date":"5 June 2016","externalUrl":null,"permalink":"/pages/","section":"Pages","summary":"","title":"Pages","type":"pages"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]